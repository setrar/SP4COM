{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77296721-a912-4744-a169-8cf8e79c7f17",
   "metadata": {},
   "source": [
    "# &#x1F4DD; REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8233d69-be14-4dd6-85cf-9777966fa064",
   "metadata": {},
   "source": [
    "# Homework &#x0031;&#xFE0F;&#x20E3; \n",
    "\n",
    "Homework policy: the homework is individual. Students are encouraged to discuss with fellow students to try to find the main structure of the solution for a problem, especially if they are totally stuck at the beginning of the problem. However, they should work out the details themselves and write down in their own words only what they understand themselves. For every answer you provide, try to give it in its simplest form, while answering correctly. Results that are available in the course notes can be used and referenced and do not need to be rederived.\n",
    "\n",
    "You can answer in French or in English. Do not forget to answer all subquestions. Word processing (Word, Latex,...) would be appreciated, or scanned readable handwritten notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee1a8b-4ed3-4612-8ac7-53262b62978a",
   "metadata": {},
   "source": [
    "#### ___Spatial Processing: Linear Interference Cancellation___\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e9426-5532-4605-abe9-0dbd74d885f7",
   "metadata": {},
   "source": [
    "### **&#x2488;** Adaptation of the Spatial ICMF via LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa06fad-a6b5-4287-82cf-af4fbc1a81c8",
   "metadata": {},
   "source": [
    "<img src=images/ICMF_via_LMS.png width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c408472-99f7-4ab4-b4d0-2a6af597e222",
   "metadata": {},
   "source": [
    "Consider adaptation of the spatial Interference Canceling Matched Filter (ICMF) depicted in the figure above. The received signal $y[k]$ contains m subchannels and the interference canceling filter $\\mathbf{f}$ is represented as a row vector. For a generic value of $\\mathbf{f}$, we get the error signal \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k](\\mathbf{f}) = d[k]− \\mathbf{f} \\; \\mathbf{x}[k] . \\qquad\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For the adaptation of $\\mathbf{f}$, the error signal is $\\epsilon[k]$ which also provides an estimate of the transmitted symbol sequence $a[k]$ (up to a scale factor $\\|\\mathbf{h}\\|^2$), the desired response signal is $d[k]$, which is the spatial matched filter output $\\mathbf{y}_1[k]$, and the input signal is $\\mathbf{x}[k]$, which is also the output $\\mathbf{y}_2[k]$ of the orthogonal complement filter $\\mathbf{h}^{\\perp H}$. The transmitted symbol sequence $a[k]$ and the additive noise sequence $\\mathbf{v}[k]$ are both considered to be temporally white and mutually independent, whereas the noise is spatially colored with covariance matrix $R_\\mathbf{VV}$ (the noise could contain interference). The transmitted power is $\\sigma_a^2$. We assume in a first instance that $\\hat{\\mathbf{h}}[k] = \\mathbf{h}$ (and hence $\\hat{\\mathbf{h}}^\\perp[k] = \\mathbf{h}^\\perp$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a07c8-493f-4581-a5a0-9ae15cb1816d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x5F;)** ___Diagram Interpretation___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8965d-36d1-4101-95e8-fd249afdb933",
   "metadata": {},
   "source": [
    "To express all the signal functions present in the diagram, let's analyze the flow and components step by step.\n",
    "\n",
    "1. **Input Signal**: $a[k] $ This is the input signal to the system.\n",
    "2. **Convolution with $ \\mathbf{h} $**: $ y[k] = h * a[k] + v[k] $: The input signal $ a[k] $ is convolved with a filter $ h $, and noise $ v[k] $ is added. Here, $ y[k] $ is the resulting signal after the convolution and noise addition.\n",
    "3. **Splitting $ y[k] $ and Applying Filters**: $ y[k] $ is split into two paths. One path is passed through the filter $ h^H[k] $ and the other through $ h^{\\perp H}[k] $.\n",
    "4. **Desired Signal**: $ d[k] = y_1[k] = h^H[k] * y[k] $: The output of the `matched filter` $ h^H[k] $ applied to $ y[k] $ is the desired signal $ d[k] $.\n",
    "5. **Reference Signal**: $ x[k] = y_2[k] = h^{\\perp H}[k] * y[k] $: The output of the filter $ h^{\\perp H}[k] $ applied to $ y[k] $ is the reference signal $ x[k] $.\n",
    "6. **Adaptive Filter Output**:  $ \\hat{a}[k] = \\mathbf{f}[k] * x[k] $: The reference signal $ x[k] $ is passed through the adaptive filter $ \\mathbf{f}[k] $ to produce the estimated signal $ \\hat{a}[k] $.\n",
    "7. **Error Signal**: $ \\epsilon[k] = d[k] - \\hat{a}[k] $: The error signal $ \\epsilon[k] $ is the difference between the desired signal $ d[k] $ and the estimated signal $ \\hat{a}[k] $. $\\boxed{ \\color{green} See \\text{ << Playing with the Error Signal >> } below }$\n",
    "\n",
    "This setup is typical in adaptive filtering applications where an adaptive filter $ \\mathbf{f}[k] $ is adjusted to minimize the error $ \\epsilon[k] $ between the desired signal $ d[k] $ and the output $ \\hat{a}[k] $ of the adaptive filter. The filters $ h^H[k] $ and $ h^{\\perp H}[k] $ are used to decompose the signal $ y[k] $ into components that can be separately processed to achieve the desired filtering and error minimization.\n",
    "\n",
    "### Playing with the Error Signal\n",
    "\n",
    "Given the diagram structure and the common practices in adaptive filtering, the $ \\epsilon[k] = d[k] - \\hat{a}[k] $ formulation is typically used to adapt the filter $\\mathbf{f}[k]$ to minimize the difference between the desired signal and the estimated interference. This helps in improving the desired signal's quality.\n",
    "\n",
    "Meawhile the diagram suggests $ \\epsilon[k] = \\hat{a}[k] $ which indicates that the filter output itself is considered as the error, which might imply that the primary focus is on the filter output (perhaps for monitoring or secondary processing).\n",
    "\n",
    "### Interference Cancellation && Interference Cancelling\n",
    "\n",
    "While the terms interference cancellation and interference cancelling are often used interchangeably, they can imply slightly different aspects of the process. Interference cancellation is a more comprehensive term covering the entire process of dealing with interference, whereas interference cancelling focuses on the specific, often $\\color{salmon}\\text{real-time}$, techniques used to mitigate interference.\n",
    "\n",
    "The primary difference lies in the interpretation and application:\n",
    "- **$ \\epsilon[k] = d[k] - \\hat{a}[k] $**: This is used for adaptive filtering to minimize the difference between the desired signal and the filter output, common in Least Mean Squares (LMS) algorithms.\n",
    "- **$\\color{salmon} \\epsilon[k] = \\hat{a}[k] $**: This treats the filter output as the error signal itself, often seen in `interference` or noise `cancellation contexts` where the goal is to adapt the filter to minimize the interference component.\n",
    "\n",
    "For clarity and correctness in our specific application, we will ensure to cross-check with the context provided by the diagram and any accompanying documentation. \n",
    "\n",
    "In the subsequent sections, the `Error Signal` $\\epsilon$ will change form:\n",
    "- $\\epsilon[k](f) = d[k] - \\mathbf{f} \\, \\mathbf{x}[k]$ for a generic value of $\\mathbf{f}$\n",
    "- $e[k] = \\epsilon[k](\\mathbf{f}^o)$ for the optimal error signal. Note the $e$, not $\\epsilon$\n",
    "- $e[k] = d[k] - \\mathbf{f} \\, \\mathbf{x}[k]$ for the unperturbed $e^o[k]$\n",
    "- let's make sure we keep track of the right notation by trying to give a context wherever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980940e8-cdd3-40ab-9998-8b269c4d3489",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x61;)** ___LMMSE design___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc23a9-c019-42b3-a560-92b77157b68d",
   "metadata": {},
   "source": [
    "Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$.\n",
    "Let $e[k] = \\epsilon[k](\\mathbf{f}^o)$ be the optimal error signal. Derive an expression for $e[k]$ in terms of the quantities in the figure.\n",
    "\n",
    "Introduce the matrix square root $R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}$ and the transformed quantities\n",
    "$\\mathbf{h}^{'} = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}$. Note that if $R_{\\mathbf{VV}}$ is not a multiple of identity, then $h'$ and $h^{\\perp '}$ are no longer orthogonal. Also introduce $\\mathbf{v}^′[k] = R_{\\mathbf{VV}}^{−1/2}\\mathbf{v}[k]$ for which $R_{\\mathbf{V'V'}} = I_m$ . Find now a simplified expression for $e[k]$ and show that the corresponding MMSE is\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_e^2 = \\sigma_a^2 \\| \\mathbf{h} \\|^4 + \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2\n",
    "\\end{equation}\n",
    "$$\n",
    "where $P_{\\mathbf{g}} = \\mathbf{g}(\\mathbf{g}^H \\mathbf{g})^{−1}\\mathbf{g}^H , P_{\\mathbf{g}}^{\\perp} = I − P_{\\mathbf{g}}$ are the projection matrices on the column space of $\\mathbf{g}$ and its orthogonal complement respectively. When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81885a-cebb-4f3e-8591-6f7a3752248a",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eade45-bc3b-4da7-9af6-d47d4888cf09",
   "metadata": {},
   "source": [
    "To derive the Linear Minimum Mean Square Error (LMMSE) filter in a condensed form that efficiently incorporates the spatial properties of channel vectors and the noise covariance matrix, let's follow a straightforward approach:\n",
    "\n",
    "***Overview***\n",
    "\n",
    "The LMMSE filter aims to minimize the expected square error between the desired response and the output of a filter applied to an input signal. This filter accounts for the signal and noise characteristics to optimize signal estimation.\n",
    "\n",
    "In mathematical terms, the LMMSE filter is obtained by solving the following optimization problem: $ \\mathbf{f}^o = \\underset{\\mathbf{f}}{\\text{arg min}} \\; E\\left[ \\left( d[k] - \\mathbf{f} \\mathbf{x}[k] \\right)^2 \\right] $ Where $E[\\cdot]$ denotes the expectation operator.\n",
    "\n",
    "***Derivation Steps***\n",
    "\n",
    "- ##### **Step 1: Define Variables**\n",
    "  \n",
    "- **Received Signal $ \\mathbf{y}[k] $**: Composed of the transmitted signal affected by the channel $ \\mathbf{h} $ and noise $ \\mathbf{v}[k] $.\n",
    "  $\n",
    "  \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k]\n",
    "  $\n",
    "\n",
    "- **Desired Response $ d[k] $**: The output from the matched filter targeting the component in the direction of $ \\mathbf{h} $.\n",
    "  $\n",
    "  d[k] = \\mathbf{h}^H \\mathbf{y}[k]\n",
    "  $\n",
    "\n",
    "- **Input Signal $ \\mathbf{x}[k] $**: Extracts the component orthogonal to $ \\mathbf{h} $.\n",
    "  $\n",
    "  \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}[k]\n",
    "  $\n",
    "\n",
    "- ##### **Step 2: Calculate Covariance**\n",
    "\n",
    "- **Auto-Covariance $ R_{\\mathbf{xx}} $** and **Cross-Covariance $ R_{d\\mathbf{x}} $** based on $ \\mathbf{x}[k] $ and $ d[k] $:\n",
    "  $\n",
    "  R_{\\mathbf{xx}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp, \\quad R_{d\\mathbf{x}} = \\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp\n",
    "  $\n",
    "\n",
    "- ##### **Step 3: LMMSE Filter Formula**\n",
    "  \n",
    "- The filter that minimizes the mean squared error:\n",
    "  $\n",
    "  \\mathbf{f}^o = R_{d\\mathbf{x}} R_{\\mathbf{xx}}^{-1}\n",
    "  $\n",
    "  Substituting the derived covariance expressions, we obtain:\n",
    "\n",
    "  $ {\\color{salmon} \\framebox[1][10]{ Solution: } }$ \n",
    "\n",
    "  $\n",
    "  \\boxed{\n",
    "  \\mathbf{f}^o = (\\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp) (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1}\n",
    "  }\n",
    "  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d24036-c37e-4d16-8295-cc0dd07d472e",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Derive an expression for $e[k]$ in terms of the quantities in the figure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9ba52-27fc-4e27-bdde-d7ae9a18df29",
   "metadata": {},
   "source": [
    "Given the transformation involving the matrix square root of the noise covariance matrix $ R_{\\mathbf{VV}} $ and its relation to the transformed quantities, we can derive an updated expression for $ e[k] $ in this new transformed space. The aim is to simplify and normalize the relationships by transforming the channel vector and noise vector such that the new noise vector $ \\mathbf{v}'[k] $ has a covariance matrix of $ I_m $, the identity matrix.\n",
    "\n",
    "- #### Transformations Introduced:\n",
    "\n",
    "1. **Noise Covariance Square Root**:\n",
    "   $\n",
    "   R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}\n",
    "   $\n",
    "2. **Transformed Channel Vector**:\n",
    "   $\n",
    "   \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}\n",
    "   $\n",
    "3. **Transformed Noise Vector**:\n",
    "   $\n",
    "   \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2}\\mathbf{v}[k]\n",
    "   $\n",
    "   where $ R_{\\mathbf{V'V'}} = I_m $.\n",
    "\n",
    "#### Step 1: Redefine the Projections and Signal Components\n",
    "\n",
    "Under these transformations, the relationship between $ d[k] $, $ \\mathbf{x}[k] $, and the projections of vectors changes:\n",
    "- **Received Signal** $ \\mathbf{y}[k] $:\n",
    "  $\n",
    "  \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k] = R_{\\mathbf{VV}}^{-1/2} (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Desired Signal** $ d[k] $:\n",
    "  $\n",
    "  d[k] = \\mathbf{h}^H \\mathbf{y}[k] = (\\mathbf{h}')^H R_{\\mathbf{VV}}^{-1/2} \\mathbf{y}[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Input Signal** $ \\mathbf{x}[k] $:\n",
    "  $\n",
    "  \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}[k]\n",
    "  $\n",
    "  We need to define $ \\mathbf{h}^{\\perp '} $ such that it is orthogonal in the transformed space, which requires additional analysis or assumptions since $ \\mathbf{h}' $ and $ \\mathbf{h}^{\\perp '} $ are not necessarily orthogonal due to the non-identity $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "#### Step 2: Expression for $ e[k] $ in the Transformed Space\n",
    "\n",
    "Assuming we have derived or defined an appropriate $ \\mathbf{h}^{\\perp '} $:\n",
    "$ \\mathbf{x}[k] = (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "$ e[k] = d[k] - \\mathbf{f}^o \\mathbf{x}[k] $\n",
    "where $ \\mathbf{f}^o $ needs to be recalculated for the transformed space:\n",
    "$ \\mathbf{f}^o = (\\mathbf{h}'^H \\mathbf{h}^{\\perp '}) ((\\mathbf{h}^{\\perp '})^H \\mathbf{h}^{\\perp '})^{-1} $\n",
    "\n",
    "Then:\n",
    "$ e[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) - \\mathbf{f}^o (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "This describes the error in terms of the transformed channel $ \\mathbf{h}' $ and noise $ \\mathbf{v}' $, where:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "#### Step 3: Rewriting $ e[k] $ in Original Terms\n",
    "\n",
    "To rewrite $ e[k] $ using the original $ \\mathbf{h} $ and $ \\mathbf{v} $, we need to express the transformed variables back in terms of the originals:\n",
    "\n",
    "- ##### a). **Transform Back the Noise and Channel Vectors**\n",
    "- **Transformed Channel**:\n",
    "  $\n",
    "  \\mathbf{h}' a[k] = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} a[k]\n",
    "  $\n",
    "\n",
    "  Multiplied through the error signal expression, this becomes:\n",
    "  $\n",
    "  (\\mathbf{h}')^H \\mathbf{h}' a[k] = \\mathbf{h}^H R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} \\mathbf{h} a[k] = \\mathbf{h}^H \\mathbf{h} a[k]\n",
    "  $\n",
    "  showing that multiplication with the square root and its Hermitian transpose returns to the original form because $ R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} = R_{\\mathbf{VV}} $ and $ \\mathbf{h} $ is normalized or adjusted accordingly.\n",
    "\n",
    "- **Transformed Noise**:\n",
    "  $\n",
    "  \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "  \n",
    "  Using the projection operator:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}} \\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k]\n",
    "  $\n",
    "  This matrix is essentially what happens when we multiply the noise by the orthogonal projection matrix adjusted by the covariance matrix $ R_{\\mathbf{VV}} $. It projects $ \\mathbf{v}[k] $ onto the space orthogonal to $ \\mathbf{h}^{\\perp} $ under the metric induced by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "- ##### b). **Combine Back into the Error Equation**\n",
    "\n",
    "  After substituting these transformations back and simplifying:\n",
    "\n",
    "  $ {\\color{salmon}  \\framebox[1][10]{ Solution: } }$\n",
    "\n",
    "  $\\boxed{\n",
    "  e[k] = \\mathbf{h}^H (\\mathbf{h} \\, a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])\n",
    "  }\n",
    "  $\n",
    "\n",
    "  This result shows the a priori error, which accounts for the noise's coloring through $ R_{\\mathbf{VV}} $ and  effectively reduces the noise components aligned with the space spanned by $ \\mathbf{h}^\\perp $ under this coloring. The transformation retains the adaptive filter's capability to handle spatially colored noise and interference correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242acd6b-a0f9-4dbf-bb82-cca03d7d65de",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Finding a simplified expression for $e[k]$ and showing the corresponding MMSE\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f56dd-d1d1-4f1a-8689-deeb4a4cf796",
   "metadata": {},
   "source": [
    "To transition from the complex expression for $ e[k] $ involving the projection matrix on $ \\mathbf{h}^\\perp $ in the original space to a simplified expression involving transformations and projection matrices in the transformed space, we need to map each term appropriately and understand how the transformations and projections relate. Let’s break down this transition step-by-step:\n",
    "\n",
    "___Starting Point___\n",
    "\n",
    "The initial formula:\n",
    "$ \n",
    "e[k] = \\mathbf{h}^H (\\mathbf{h} \\, a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])\n",
    "$\n",
    "\n",
    "This formula accounts for:\n",
    "1. **Signal Component:** $ \\mathbf{h}^H \\mathbf{h} a[k] $\n",
    "2. **Noise Component:** Modified by a projection that reduces noise components along the space spanned by $ \\mathbf{h}^\\perp $, adjusted by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "___Transforming the Terms___\n",
    "\n",
    "1. **Signal Component**\n",
    "\n",
    "- The signal component $\\mathbf{h}^H \\mathbf{h} \\, a[k]$ is straightforward. It simplifies to $\\|\\mathbf{h}\\|^2 a[k]$ since $\\mathbf{h}^H \\mathbf{h}$ is the power of $\\mathbf{h}$, or the square of its norm.\n",
    "\n",
    "2. **Noise Component**\n",
    "\n",
    "- The original noise term uses a projection to eliminate components of noise in the subspace spanned by $ \\mathbf{h}^\\perp $ adjusted by $ R_{\\mathbf{VV}} $. To transform this into the desired format:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k] \n",
    "  $\n",
    "  This term can be seen as a projection of $ \\mathbf{v}[k] $ onto the orthogonal complement of $ \\mathbf{h}^\\perp $ in the metric of $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "To connect this with the transformed version $ \\mathbf{h}' $ and $ \\mathbf{v}' $, recall:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "Thus, rewriting the projection matrix for the transformed variables:\n",
    "- We use $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} = I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H} $ where $ \\mathbf{h}^{\\perp '} = R_{\\mathbf{VV}}^{H/2} \\mathbf{h}^\\perp $.\n",
    "- Applying this to $ \\mathbf{v}'[k] $ gives:\n",
    "  $\n",
    "  \\mathbf{h}'^H \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k] = \\mathbf{h}'^H \\Big[I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H}\\Big] R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "\n",
    "___Conclusion___\n",
    "\n",
    "This translates the projection and transformation of the noise vector into the following expression:\n",
    "\n",
    "$ {\\color{salmon}  \\framebox[1][10]{ Solution: } }$\n",
    "\n",
    "$\\boxed{\n",
    "e[k] = \\|\\mathbf{h}\\|^2 a[k] + \\mathbf{h}^{'H} \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k]\n",
    "}$\n",
    "\n",
    "This simplified formula connects the original space properties $ \\mathbf{h} $, $ \\mathbf{v}[k] $ to their transformed versions under $ R_{\\mathbf{VV}} $, reflecting how signal processing can adaptively minimize noise effects while maintaining focus on the desired signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9408b-0e30-4e0a-9bf2-23816b8d0fa5",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03c444-786e-4dbe-95d8-aacaeead3589",
   "metadata": {},
   "source": [
    "To clarify and simplify the explanation regarding the expression $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ when $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, let's focus on a straightforward reformulation:\n",
    "\n",
    "___Background___\n",
    "\n",
    "Given the original transformations and projection matrices:\n",
    "- **Channel Transformation**: $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $\n",
    "- **Projection Matrices**: For a vector $ \\mathbf{g} $,\n",
    "  - $ \\mathit{P}_{\\mathbf{g}} = \\mathbf{g} (\\mathbf{g}^H \\mathbf{g})^{-1} \\mathbf{g}^H $\n",
    "  - $ \\mathit{P}_{\\mathbf{g}}^{\\perp} = I - \\mathit{P}_{\\mathbf{g}} $\n",
    "\n",
    "___Key Calculation___\n",
    "\n",
    "- The projection $ \\mathit{P}_{\\mathbf{h}}^{\\perp} $ projects onto the space orthogonal to $ \\mathbf{h} $.\n",
    "- Applying this to $ \\mathbf{h}' $, we find that:\n",
    "  $ \\mathit{P}_{\\mathbf{h}}^{\\perp} \\mathbf{h}' = \\mathit{P}_{\\mathbf{h}}^{\\perp} (\\sigma_v \\mathbf{h}) = 0 $\n",
    "  because $ \\mathbf{h}' $ is a scalar multiple of $ \\mathbf{h} $ and lies entirely within the space spanned by $ \\mathbf{h} $.\n",
    "\n",
    "___Simplification___\n",
    "\n",
    "Given the transformation $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ actually does not modify $ \\mathbf{h}' $ because it's directly aligned with $ \\mathbf{h} $. Thus, any projection onto $ \\mathbf{h} $ or away from $ \\mathbf{h} $ will not change $ \\mathbf{h}' $:\n",
    "\n",
    "- The expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ effectively simplifies to:\n",
    "  $ \\| \\sigma_v \\mathbf{h} \\|^2 $\n",
    "  by recognizing that $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' $ is simply $ \\mathbf{h}' $ itself (no component is removed by the projection):\n",
    "\n",
    "\n",
    "  $ {\\color{salmon}  \\framebox[1][10]{ Solution: } }$\n",
    "\n",
    "\n",
    "  $ \\boxed{\n",
    "  \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 = \\| \\sigma_v \\mathbf{h} \\|^2 = \\sigma_v^2 \\| \\mathbf{h} \\|^2   \n",
    "  }$\n",
    "\n",
    "___Conclusion___\n",
    "\n",
    "This simplification highlights that when $ R_{\\mathbf{VV}} = \\sigma_v^2 I_m $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ retains the full transformed channel vector $ \\mathbf{h}' $ because the transformation does not introduce any components orthogonal to $ \\mathbf{h} $ itself. Therefore, the expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ evaluates to $ \\sigma_v^2 \\| \\mathbf{h} \\|^2 $, representing the power of the channel in the presence of isotropic noise scaled by $ \\sigma_v^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd51b1-ba26-4b78-aafb-d7816621fc3d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x62;)** ___LMS adaptation of $f$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca82880-fbd8-4b5c-9376-4c8962122c94",
   "metadata": {},
   "source": [
    "The &#x1F449; LMS algorithm consists of applying one iteration, per sampling period, of the steepest- descent strategy to the instantaneous error criterion $|\\epsilon[k](\\mathbf{f})|^2 = \\epsilon^*[k](\\mathbf{f}) \\, \\epsilon[k](\\mathbf{f})$. In the complex signals case, this becomes\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{f}[k] = \\left.\\mathbf{f}[k - 1] - \\mu \\frac{\\partial|\\epsilon[k](\\mathbf{f})|^2}{\\partial{\\mathbf{f^*}}}\\right|_{\\mathbf{f}=\\mathbf{f}[k-1]}\n",
    "\\end{equation}\n",
    "$$\n",
    "Work out the gradient term in this LMS update. We shall simplify the notation for the a priori error signal as $\\epsilon[k] = \\epsilon[k](\\mathbf{f}[k - 1])$.\n",
    "To check that the gradient has indeed to be taken w.r.t. $\\mathbf{f}^∗$ (and not $\\mathbf{f}$), express the a posteriori error signal $\\epsilon[k](\\mathbf{f}[k])$ as a function of the a priori error signal and observe that the update leads to a smaller error signal for a proper choice of stepsize $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780709f2-5917-4297-add3-c7b952f55153",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Derive the gradient term in the LMS (Least Mean Squares) algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5111f9-0375-4a86-bab3-4d9613e29fe5",
   "metadata": {},
   "source": [
    "To derive the Least Mean Squares (LMS) algorithm with respect to complex signals, particularly addressing the computation of the gradient term required for the update and the significance of using the conjugate transpose, here is a compact review:\n",
    "\n",
    "___Context and Equation___\n",
    "\n",
    "The LMS algorithm updates the filter coefficients by applying a steepest-descent method to minimize the instantaneous error criterion, which for complex signals involves the complex conjugate: $ |\\epsilon[k](\\mathbf{f})|^2 = \\epsilon^*[k](\\mathbf{f}) \\epsilon[k](\\mathbf{f}) $ where the error $ \\epsilon[k] $ at iteration $ k $ is defined as:\n",
    "\n",
    "  $  {\\color{salmon} \\framebox[1][10]{ a priori error signal : } }$\n",
    "\n",
    "\n",
    "$\\boxed{\n",
    "\\epsilon[k] = d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k] \n",
    "}$\n",
    "\n",
    "Here, $ d[k] $ is the desired signal, $ \\mathbf{x}[k] $ is the input, and $ \\mathbf{f}[k-1] $ are the filter coefficients from the previous iteration.\n",
    "\n",
    "___Gradient Computation___\n",
    "\n",
    "To update the coefficients effectively, the gradient of the error squared with respect to the conjugate of the filter coefficients ($\\mathbf{f}^*$) must be computed. This is because the filter output depends linearly on the conjugate of the coefficients in the complex-valued domain.\n",
    "\n",
    "The gradient of $ |\\epsilon[k]|^2 $ with respect to $ \\mathbf{f}^* $ is: $ \\frac{\\partial |\\epsilon[k]|^2}{\\partial \\mathbf{f}^*} = \\frac{\\partial}{\\partial \\mathbf{f}^*} \\left((d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])^*(d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])\\right) $\n",
    "\n",
    "Breaking down the product and applying the derivative yields: $ \\frac{\\partial |\\epsilon[k]|^2}{\\partial \\mathbf{f}^*} = -\\mathbf{x}[k] \\epsilon^*[k] $\n",
    "\n",
    "___Update Rule___\n",
    "\n",
    "Incorporating the derived gradient into the update equation provides the rule for adjusting the filter coefficients:\n",
    "\n",
    "  $   {\\color{salmon} \\framebox[1][10]{ update equation solution: } }$\n",
    "\n",
    "$\\boxed{\n",
    "\\mathbf{f}[k] = \\mathbf{f}[k-1] + \\mu \\epsilon[k] \\mathbf{x}^H[k] \n",
    "}$\n",
    "\n",
    "This update ensures the filter coefficients are adjusted in a manner that minimizes the mean squared error. The term $\\mathbf{x}^H[k]$ represents the conjugate transpose of the input vector, which correctly aligns the dimensions and conjugate pairs for the update in the complex domain.\n",
    "\n",
    "___Validation of the Update___\n",
    "\n",
    "To validate that this update leads to a reduction in the error for an appropriate choice of step size ($\\mu$), \n",
    "- consider the a posteriori error signal after applying the update: $ \\epsilon[k](\\mathbf{f}[k]) = d[k] - \\mathbf{f}[k] \\mathbf{x}[k] $\n",
    "- Substituting the update equation: $ \\epsilon[k](\\mathbf{f}[k]) = d[k] - (\\mathbf{f}[k-1] + \\mu \\epsilon[k] \\mathbf{x}^H[k]) \\mathbf{x}[k] $\n",
    "\n",
    "   $ {\\color{salmon} \\framebox[1][10]{ a posteriori error signal solution: } }$\n",
    "\n",
    "   $\\boxed{\n",
    "   \\epsilon[k](\\mathbf{f}[k]) = \\epsilon[k] - \\mu \\epsilon[k] \\|\\mathbf{x}[k]\\|^2 = ( 1 - \\mu \\|\\mathbf{x}[k]\\|^2 ) \\epsilon[k]\n",
    "   }$\n",
    "\n",
    "   The term $\\mu \\epsilon[k] \\|\\mathbf{x}[k]\\|^2$ indicates that if $\\mu$ is chosen small enough, the error magnitude $|\\epsilon[k](\\mathbf{f}[k])|^2$ can indeed be reduced compared to $|\\epsilon[k]|^2$, confirming the efficacy of the update rule in reducing the error. This adjustment aligns with the steepest descent approach, optimized for complex signals to ensure convergence towards the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73706076-74b0-47e4-b005-3f4745c42447",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x63;)** ___Steady-state analysis of LMS adaptation of $\\mathbf{f}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210e57-0550-442c-b96c-8cbd624a8e12",
   "metadata": {},
   "source": [
    "Let $\\mathbf{\\tilde{f}}[k] = \\mathbf{f}^o − \\mathbf{f}[k]$ be the filter error. Note that due to the presumed temporal whiteness of\n",
    "$a[k]$ and $\\mathbf{v}[k]$, also $d[k]$ and $\\mathbf{x}[k]$ are temporally white. Hence $\\mathbf{\\tilde{f}}[k−1]$ and $e[k]$ are independent\n",
    "(strictly speaking only uncorrelated).\n",
    "\n",
    "Let $R_{\\mathbf{\\tilde{f}}\\mathbf{\\tilde{f}}}[k] = \\mathrm{E} \\, \\mathbf{\\tilde{f}}^H[k] \\, \\mathbf{\\tilde{f}}[k]$. We can write for the a priori error signal and MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k] = e[k] + \\mathbf{\\tilde{f}}[k−1] \\mathbf{x}[k] \\implies \\underbrace{\\sigma_{\\epsilon[k]}^2}_\\text{MSE} = \\underbrace{\\sigma_e^2}_\\text{MMSE} + \\underbrace{tr\\{R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] \\, R_{\\mathbf{XX}} \\}}_\\text{EMSE}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The LMS update (3) for $\\mathbf{f}$ leads to the following recursion for $\\mathbf{\\tilde{f}}[k]$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, \\mathbf{x}[k] \\, \\mathbf{x}^H[k]) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which, using the averaging analysis for small stepsize, can be approximated by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, R_{\\mathbf{XX}}) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "From (6), obtain the time evolution for the filter error correlation matrix $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k]$. Assume\n",
    "$\\mu$ small so that $I − \\mu R_{\\mathbf{xx}}$ is stable and neglect second-order terms in $\\mu$. In steady-state $\\sigma_{\\epsilon[\\infty]}^2 = \\sigma_{\\epsilon}^2$ and $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[\\infty] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}$  . Show now from (6) that we obtain for the steady-state Excess MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "EMSE = \\frac{\\mu}{2} \\, \\sigma_e^2 \\, tr\\{R_{\\mathbf{XX}} \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note that $R_{\\mathbf{XX}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{\\perp}$ is not Toeplitz and its diagonal elements are not all equal. Using the simplified expression derived in `(a)` for $e[k]$, show the corresponding expression for the a priori error signal $\\epsilon[k]$.\n",
    "\n",
    "The signal part in $\\epsilon[k]$ is the term containing $a[k]$ and all the rest is noise. Using the expression for MMSE in `(2)`, derive an expression for the SNR in $\\epsilon[k]$. Note that the noise term contains a signal part which limits the SNR attainable by the adaptive system. This is due to the fact that the signal part in the error signal $e[k]$ acts like noise for the adaptation of the filter $\\mathbf{f}[k]$. This problem is generic for any adaptation algorithm and not just specific for LMS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f98572-236e-4c79-a699-b96f20c0d456",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** obtain the time evolution for the filter error correlation matrix $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k]$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca65072-4767-4ded-a519-efbad0756a27",
   "metadata": {},
   "source": [
    "To derive the time evolution of the filter error correlation matrix $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $ from the update equation of the filter error $ \\mathbf{\\tilde{f}}[k] $, we'll start by plugging the recursive definition into the expression for the correlation matrix, and then applying expectations to simplify terms.\n",
    "\n",
    "- ##### Step 1: Update Equation for $ \\mathbf{\\tilde{f}}[k] $\n",
    "Given:\n",
    "$ \n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\n",
    "$\n",
    "\n",
    "- ##### Step 2: Defining $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $\n",
    "The error correlation matrix $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $ is defined as:\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \\mathrm{E}[\\mathbf{\\tilde{f}}[k] \\mathbf{\\tilde{f}}^H[k]]\n",
    "$\n",
    "\n",
    "- ##### Step 3: Substitute $ \\mathbf{\\tilde{f}}[k] $ into the Correlation Matrix\n",
    "Substituting the update equation into the correlation matrix definition:\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \\mathrm{E}\\left[\\left(\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\\right) \\left(\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\\right)^H\\right]\n",
    "$\n",
    "\n",
    "- ##### Step 4: Expand the Expectation\n",
    "\n",
    "Expanding the expression within the expectation:\n",
    "\n",
    "$ \n",
    "\\begin{flalign*}\n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \n",
    "    \\mathrm{E}\\left[\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) (I - \\mu R_{\\mathbf{XX}})^H \\mathbf{\\tilde{f}}^H[k - 1]\\right] \n",
    "    \\\\\n",
    "    - \\mathrm{E}\\left[\\mu \\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) e^*[k] \\mathbf{x}[k]\\right] \n",
    "    \\\\\n",
    "    - \\mathrm{E}\\left[\\mu e[k] \\mathbf{x}^H[k] (I - \\mu R_{\\mathbf{XX}})^H \\mathbf{\\tilde{f}}^H[k - 1]\\right] \n",
    "    \\\\\n",
    "    + \\mathrm{E}\\left[\\mu^2 e[k] e^*[k] \\mathbf{x}^H[k] \\mathbf{x}[k]\\right]\n",
    "\\end{flalign*}\n",
    "$\n",
    "\n",
    "- ##### Step 5: Simplify Using Independence and Neglecting Higher Order Terms\n",
    "Assuming $ \\mathbf{\\tilde{f}}[k - 1] $ and $ e[k] $ are uncorrelated and neglecting higher-order terms in $ \\mu $,\n",
    "\n",
    "  $  {\\color{salmon}  \\framebox[1][10]{ the equation simplifies to: } } $\n",
    "\n",
    "\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] - \\mu R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}} - \\mu R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] + \\mu^2 \\sigma_e^2 R_{\\mathbf{XX}}\n",
    "$\n",
    "\n",
    "Neglegted term: $+ \\mu^2 R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}}$ 4th term\n",
    "\n",
    "### Conclusion\n",
    "This equation describes how the error correlation matrix evolves over time under the influence of the input correlation matrix $ R_{\\mathbf{XX}} $ and the learning rate $ \\mu $. It provides insight into the stability and convergence characteristics of the adaptive filter, highlighting the impact of step size and input properties on the filter's learning dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db7fa3-fff1-4fb3-88bd-7f1887de682b",
   "metadata": {},
   "source": [
    "##### Definitions:\n",
    "\n",
    "- **Updated Equation**: Focuses on how the error evolves and reacts to ongoing learning and adaptation, used during the active phase of filter training and initial deployment.\n",
    "- **Steady-State Equation**: Provides insights into the performance and behavior of the filter after it has adapted sufficiently to the statistics of the inputs and noise, important for evaluating final filter settings and design parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826ed87-ca8a-42c0-8503-a39929ee9932",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee90f58-beee-419d-887b-688e66cecb98",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x64;)** ___Steady-state analysis of signal compensated LMS adaptation of $\\mathbf{f}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f70cf-4f6a-4aa3-a036-13eee94f6522",
   "metadata": {},
   "source": [
    "Consider now compensating the signal part in the desired response signal $d[k]$ for the LMS adaptation. So we shall take as desired response $d[k] = \\mathbf{h}^H (\\mathbf{y}[k] − \\mathbf{h} \\, a[k]) = \\mathbf{h}^H \\, \\mathbf{v}[k]$. The goal of the receiver is to detect the $a[k]$ which are hence unknown. The way this signal compensation can be implemented then is by either limiting the update for f to time instants at which the $a[k]$ are training symbols (used for estimating h also, see further) or by using the detected $a[k]$ (decision-directed (DD) strategy). In the DD strategy, the symbol $a[k]$ gets detected from $\\mathbf{h}^H \\, \\mathbf{y}[k] − \\mathbf{f}[k - 1]$ (or delay needs to be introduced for the updating of f if also channel decoding gets exploited to get more reliable $a[k]$). Making abstraction of these details, consider hence $d[k] = \\mathbf{h}^H \\, \\mathbf{v}[k]$.\n",
    "\n",
    "Does the signal compensation influence the optimal filter setting $\\mathbf{f}^o$? What do the optimal error signal $e[k]$ and associated MMSE $\\sigma_e^2$ become?\n",
    "\n",
    "The signal compensation only gets done for the adaptation of $\\mathbf{f}$. The thus adapted $\\mathbf{f}$ then gets used in the original ICMF circuit. So, at the output of the ICMF, with the adapted $\\mathbf{f}$, what does the SNR become? With the signal compensation, the SNR degradation due to the adaptation of $\\mathbf{f}$ can be made arbitrarily small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a569-d0fa-4dfe-adbe-10c71c84c142",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Does the signal compensation influence the optimal filter setting $f^0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca123d9-1093-4908-9d87-5bbdcdc5e820",
   "metadata": {},
   "source": [
    "To derive the Signal-to-Noise Ratio (SNR) formula provided, we'll follow a systematic approach to combine elements like signal power, noise power, and the effect of adaptive filtering. Let's begin by identifying and understanding each component in the expression:\n",
    "\n",
    "### SNR Definition\n",
    "In signal processing, SNR is typically defined as the ratio of the power of a signal (useful information) to the power of the noise (undesired interference), affecting the fidelity of its representation. The given SNR expression can be broken down into several key parts:\n",
    "\n",
    "1. **Signal Power ($ \\| \\mathbf{h} \\|^4 \\sigma_a^2 $)**:\n",
    "   - $ \\sigma_a^2 $ represents the power of the transmitted signal symbols $ a[k] $.\n",
    "   - $ \\| \\mathbf{h} \\|^4 $ suggests an amplification factor from the channel characteristics, possibly implying some form of non-linear effect or a squared measure when considering real and imaginary components in a communication system.\n",
    "\n",
    "2. **Noise Power ($ \\| P_{\\mathbf{h}^{\\perp '} }^{\\perp} \\mathbf{h}' \\|^2 $)**:\n",
    "   - $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $ transforms the channel vector by the noise covariance matrix, potentially aligning or scaling it with the noise characteristics.\n",
    "   - $ P_{\\mathbf{h}^{\\perp '} }^{\\perp} $ is the projection matrix onto the subspace orthogonal to the orthogonal complement of $ \\mathbf{h}' $, essentially calculating the component of $ \\mathbf{h}' $ that is not orthogonal to itself. This represents how much of the transformed channel vector contributes to the noise after filtering.\n",
    "   - The norm squared ($ \\| \\cdot \\|^2 $) indicates the power contribution of this component.\n",
    "\n",
    "3. **Filter Adjustment Factor ($ \\frac{\\mu}{2} tr \\{ R_{\\mathbf{XX}} \\} $)**:\n",
    "   - $ \\mu $ is the adaptation step size in the LMS algorithm, affecting how quickly the filter adapts to changes.\n",
    "   - $ tr \\{ R_{\\mathbf{XX}} \\} $ is the trace of the input signal autocorrelation matrix, representing the total power in the input signal across all its dimensions or channels.\n",
    "   - This term adjusts the noise power term to account for the impact of the learning rate and the power of the input signals, indicating that higher adaptation rates or more powerful inputs increase the effective noise power, reducing SNR.\n",
    "\n",
    "### Putting It All Together\n",
    "The SNR formula can now be written as:\n",
    "$\n",
    "\\text{SNR} = \\frac{\\| \\mathbf{h} \\|^4 \\sigma_a^2}{\\| P_{\\mathbf{h}^{\\perp '} }^{\\perp} \\mathbf{h}' \\|^2 ( 1 + \\frac{\\mu}{2} tr \\{ R_{\\mathbf{XX}} \\})}\n",
    "$\n",
    "This equation essentially compares the modified signal power to the modified noise power, with adjustments for how the filter handles the noise (influenced by $ \\mu $ and the signal properties).\n",
    "\n",
    "### Interpretation\n",
    "- **Signal Component**: $ \\| \\mathbf{h} \\|^4 \\sigma_a^2 $ indicates a strong dependence of the SNR on the channel properties and transmitted signal power.\n",
    "- **Noise Component**: $ \\| P_{\\mathbf{h}^{\\perp '} }^{\\perp} \\mathbf{h}' \\|^2 $ modified by $ 1 + \\frac{\\mu}{2} tr \\{ R_{\\mathbf{XX}} \\} $ reflects the interaction between the filter dynamics and the signal characteristics, noting that aggressive adaptation or higher input power can degrade SNR by effectively increasing the noise power.\n",
    "\n",
    "This derived SNR formula is especially useful in adaptive filtering contexts where understanding the balance between adaptation speed, signal power, and noise characteristics is crucial for optimizing communication system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107eb07-34d6-4669-a68c-00374a4b2742",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x65;)** ___LMS adaptation of $\\mathbf{h}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12719ec-7869-4f17-a13a-2bd5d5fc8209",
   "metadata": {},
   "source": [
    "The transmitted symbols are in fact partitioned into known training symbols and actual data symbols. The training symbols get inserted periodically. They get used to adapt the channel estimate. From now on we shall denote the true value of the channel as $\\mathbf{h}^o$ (assumed time-invariant). For the adaptation of the channel estimate, consider the error signal $\\mathbf{w}[k](\\mathbf{h}) = \\mathbf{y}[k] − \\mathbf{h} \\, a[k]$. The optimal value for the error signal $\\mathbf{w}[k](\\mathbf{h}^o)$ has already been specified in the problem formulation. What is it?\n",
    "\n",
    "The LMS algorithm performs one iteration of the steepest-descent strategy per training symbol to the instantaneous error criterion $\\|\\mathbf{w}[k](\\mathbf{h})\\|^2 = \\mathbf{w}^H[k](h) \\, \\mathbf{w}[k](\\mathbf{h})$. Derive the LMS algorithm that updates the channel estimate $\\mathbf{h}[k]$ (which could have been denoted also as $\\mathbf{\\hat{h}}[k]$, but let’s keep $\\mathbf{h}[k]$). Denote the a priori error signal as $\\mathbf{w}[k]$ and the stepsize as $\\nu$. Note that the time index now is no longer the true time index but a counter for the training symbols only, since adaptation occurs only when a symbol is a training symbol.\n",
    "Develop the recursion for the channel estimation error $\\mathbf{\\hat{h}}[k] = \\mathbf{h}^o − \\mathbf{h}[k]$. Find the steady-state value for $R_\\mathbf{\\hat{h}\\hat{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85265424-113d-46ff-8340-223252b8e67e",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** What is it? (Optimal Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3ee46-822f-4d73-b355-072ba26c797b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "___Definition of the Error Signal___\n",
    "- The error signal for channel estimation, when using the channel estimate $\\mathbf{h}$, is defined as:\n",
    "  $\n",
    "  \\mathbf{w}[k](\\mathbf{h}) = \\mathbf{y}[k] - \\mathbf{h} a[k]\n",
    "  $\n",
    "- When using the optimal or true channel estimate $\\mathbf{h}^o$, this becomes:\n",
    "  $\n",
    "  \\mathbf{w}[k](\\mathbf{h}^o) = \\mathbf{y}[k] - \\mathbf{h}^o a[k]\n",
    "  $\n",
    "\n",
    "___Substituting the Received Signal___\n",
    "- Plug the expression for $\\mathbf{y}[k]$ into the error signal calculation:\n",
    "  $\n",
    "  \\mathbf{w}[k](\\mathbf{h}^o) = (\\mathbf{h}^o a[k] + \\mathbf{v}[k]) - \\mathbf{h}^o a[k]\n",
    "  $\n",
    "\n",
    "  $   \\framebox[1][10]{ the equation simplifies to: } $\n",
    "\n",
    "  $\n",
    "  \\mathbf{w}[k](\\mathbf{h}^o) = \\mathbf{v}[k]\n",
    "  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626d72c-7e8c-47c6-aeca-e75f92cf93ce",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x66;)** ___Effect of channel adaptation on LMMSE ICMF operation with long-term IC estimation___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfafe8-4c76-4e2c-a719-48982099f147",
   "metadata": {},
   "source": [
    "Consider now the use of the adapted $\\mathbf{h}[k]$ in the ICMF: for any data symbol $a[k]$, the $\\mathbf{h}$ that will be used is the one adapted with LMS at the latest training symbol before the current data symbol. The main effect is that the channel estimation error $\\mathbf{\\tilde{h}}$ will lead to signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter $\\mathbf{h}^{\\perp H}$. The effect of the error $\\mathbf{\\tilde{h}}$ on $\\mathbf{h}^\\perp$ will depend on the choice of $\\mathbf{h}^\\perp$. Assuming the error to be small, we can perform a first-order analysis of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "0 = \\mathbf{h}^{\\perp H} \\, \\mathbf{h} = (\\mathbf{h}^{o \\perp} − \\mathbf{\\tilde{h}}^{\\perp})^H (\\mathbf{h}^o − \\mathbf{\\tilde{h}}) \\approx \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}} − \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\implies \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\approx - \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\tilde{h}}^{\\perp}$ is not the orthogonal complement of $\\mathbf{\\tilde{h}}$  but the error on $\\mathbf{h}^{\\perp}$.\n",
    "\n",
    "Describe the signals $d[k]$ and $\\mathbf{x}[k]$ for the ICMF operation in terms of $\\mathbf{h}^o$, $\\mathbf{\\tilde{h}}$ and their orthogonal complement versions, and $\\mathbf{a}[k]$ and $\\mathbf{v}[k]$, neglecting products of noise terms, and using `(8)`.\n",
    "\n",
    "Find $R_{d\\mathbf{X}}$ and $R_\\mathbf{XX}$. Find the LMMSE filter $\\mathbf{f}$ in terms of the unperturbed version $\\mathbf{f}^o$.\n",
    "\n",
    "Express the corresponding error signal $e[k]$, and MMSE in terms of $\\mathbf{h}^{'} = R_\\mathbf{VV}^{H/2} \\mathbf{h}^o , \\mathbf{h}^{\\perp '} =\n",
    "R_\\mathbf{VV}^{H/2} \\mathbf{h}^{o \\perp}$. Give the increase in MSE due to the channel estimation error. How much is this increase when $R_\\mathbf{VV} = \\sigma_v^2I_m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c0286-bb0a-441d-a974-3859ec311752",
   "metadata": {},
   "source": [
    "To address the given question thoroughly, let's break it down into manageable parts and provide detailed explanations for each segment.\n",
    "\n",
    "### Signals $ d[k] $ and $ \\mathbf{x}[k] $\n",
    "\n",
    "**Desired Signal $ d[k] $**:\n",
    "\n",
    "Given:\n",
    "$ d[k] = \\mathbf{h}^H \\mathbf{y}[k] = ({\\mathbf{h}^{o}}^H - \\mathbf{\\tilde{h}}^H)(\\mathbf{h}^o \\, a[k] + \\mathbf{v}[k]) $\n",
    "\n",
    "Neglecting products of noise terms, this can be approximated as:\n",
    "$ d[k] \\approx \\|\\mathbf{h}^o \\|^2 \\, a[k] + {\\mathbf{h}^o}^H \\, \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o \\, a[k] $\n",
    "\n",
    "- $\\mathbf{h}^o$: Original (unperturbed) channel.\n",
    "- $\\mathbf{\\tilde{h}}$: Channel estimation error.\n",
    "- $a[k]$: Data symbol.\n",
    "- $\\mathbf{v}[k]$: Noise.\n",
    "\n",
    "**Reference Signal $ \\mathbf{x}[k] $**:\n",
    "\n",
    "Given:\n",
    "$ \\mathbf{x}[k] = {{\\mathbf{h}^\\perp}}^H \\mathbf{y}[k] \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] - {\\mathbf{\\tilde{h}}^\\perp}^H \\mathbf{h}^o \\, a[k] $\n",
    "\n",
    "Neglecting products of noise terms, this can be approximated as:\n",
    "$ \\mathbf{x}[k] \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] + {\\mathbf{h}^{o\\perp}}^H \\mathbf{\\tilde{h}} \\, a[k] $\n",
    "\n",
    "- $\\mathbf{h}^{o\\perp}$: Orthogonal complement of the original channel.\n",
    "- $\\mathbf{\\tilde{h}}^\\perp$: Error on $\\mathbf{h}^{\\perp}$.\n",
    "\n",
    "### Correlation Matrices $ R_{d\\mathbf{X}} $ and $ R_\\mathbf{XX} $\n",
    "\n",
    "**Cross-correlation $ R_{d\\mathbf{X}} $**:\n",
    "\n",
    "Given the signals $ d[k] $ and $ \\mathbf{x}[k] $, we have:\n",
    "$ R_{d\\mathbf{X}} = E[d[k] \\mathbf{x}^H[k]] $\n",
    "\n",
    "Substituting $ d[k] $ and $ \\mathbf{x}[k] $:\n",
    "$ d[k] \\approx \\|\\mathbf{h}^o \\|^2 a[k] + {\\mathbf{h}^o}^H \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] $\n",
    "$ \\mathbf{x}[k] \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] + {\\mathbf{h}^{o\\perp}}^H \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "Since $ \\mathbf{\\tilde{h}} $ is assumed to be small and has zero mean, we neglect higher-order noise terms and focus on dominant terms:\n",
    "$ R_{d\\mathbf{X}} \\approx \\mathbf{h}^{oH} E[\\mathbf{v}[k] \\mathbf{v}^H[k]] \\mathbf{h}^{o\\perp} $\n",
    "$ R_{d\\mathbf{X}} = \\mathbf{h}^{oH} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o\\perp} $\n",
    "\n",
    "**Autocorrelation $ R_{\\mathbf{XX}} $**:\n",
    "\n",
    "Given the signal $ \\mathbf{x}[k] $:\n",
    "$ R_{\\mathbf{XX}} = E[\\mathbf{x}[k] \\mathbf{x}^H[k]] $\n",
    "\n",
    "Substituting $ \\mathbf{x}[k] $:\n",
    "$ \\mathbf{x}[k] \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] + {\\mathbf{h}^{o\\perp}}^H \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "We consider the autocorrelation and cross terms:\n",
    "$ R_{\\mathbf{XX}} \\approx {\\mathbf{h}^{o\\perp}}^H E[\\mathbf{v}[k] \\mathbf{v}^H[k]] \\mathbf{h}^{o\\perp} + {\\mathbf{h}^{o\\perp}}^H E[\\mathbf{\\tilde{h}} a[k] a^H[k] \\mathbf{\\tilde{h}}^H] \\mathbf{h}^{o\\perp} $\n",
    "\n",
    "Given $ E[a[k] a^H[k]] = \\sigma_a^2 $:\n",
    "$ R_{\\mathbf{XX}} \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o\\perp} + \\sigma_a^2 {\\mathbf{h}^{o\\perp}}^H E[\\mathbf{\\tilde{h}} \\mathbf{\\tilde{h}}^H] \\mathbf{h}^{o\\perp} $\n",
    "\n",
    "Since $ E[\\mathbf{\\tilde{h}} \\mathbf{\\tilde{h}}^H] = R_{\\mathbf{\\tilde{h}\\tilde{h}}} $:\n",
    "$ R_{\\mathbf{XX}} = \\mathbf{h}^{o\\perp H} \\left( R_{\\mathbf{VV}} + \\sigma_a^2 R_{\\mathbf{\\tilde{h}\\tilde{h}}} \\right) \\mathbf{h}^{o\\perp} $\n",
    "\n",
    "If we assume that $ R_{\\mathbf{\\tilde{h}\\tilde{h}}} = \\nu \\mathbf{I} $ where $\\nu$ represents the variance of the channel estimation error, then:\n",
    "$ R_{\\mathbf{XX}} = \\mathbf{h}^{o\\perp H} \\left( R_{\\mathbf{VV}} + \\sigma_a^2 \\nu \\mathbf{I} \\right) \\mathbf{h}^{o\\perp} $\n",
    "$ R_{\\mathbf{XX}} = (1 + \\frac{\\nu}{2} \\sigma_a^2) \\mathbf{h}^{o\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{o\\perp} $\n",
    "\n",
    "### LMMSE Filter $ \\mathbf{f} $\n",
    "\n",
    "The LMMSE filter is given by:\n",
    "$ \\mathbf{f} = R_{d\\mathbf{X}} R_{\\mathbf{XX}}^{-1} $\n",
    "\n",
    "Substituting the expressions for $ R_{d\\mathbf{X}} $ and $ R_{\\mathbf{XX}} $:\n",
    "$ \\mathbf{f} = \\mathbf{h}^{oH} R_{\\mathbf{VV}} \\mathbf{h}^{o\\perp} \\left( (1 + \\frac{\\nu}{2} \\sigma_a^2) \\mathbf{h}^{o\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{o\\perp} \\right)^{-1} $\n",
    "$ \\mathbf{f} = \\frac{1}{1 + \\frac{\\nu}{2} \\sigma_a^2} \\left( \\mathbf{h}^{oH} R_{\\mathbf{VV}} \\mathbf{h}^{o\\perp} \\right) \\left( \\mathbf{h}^{o\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{o\\perp} \\right)^{-1} $\n",
    "$ \\mathbf{f} = \\frac{1}{1 + \\frac{\\nu}{2} \\sigma_a^2} \\mathbf{f}^o $\n",
    "\n",
    "where $ \\mathbf{f}^o $ is the unperturbed optimal filter.\n",
    "\n",
    "### Error Signal $ e[k] $\n",
    "\n",
    "Given the definitions and the filter response:\n",
    "$ e[k] = d[k] - \\mathbf{f}^H \\mathbf{x}[k] $\n",
    "\n",
    "Substitute the approximations:\n",
    "$ d[k] \\approx \\|\\mathbf{h}^o\\|^2 a[k] + {\\mathbf{h}^o}^H \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] $\n",
    "$ \\mathbf{x}[k] \\approx {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] + {\\mathbf{h}^{o\\perp}}^H \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "Then:\n",
    "$ e[k] \\approx \\|\\mathbf{h}^o\\|^2 a[k] + {\\mathbf{h}^o}^H \\mathbf{v}[k] - \\mathbf{f}^H \\left( {\\mathbf{h}^{o\\perp}}^H \\mathbf{v}[k] + {\\mathbf{h}^{o\\perp}}^H \\mathbf{\\tilde{h}} a[k] \\right) $\n",
    "\n",
    "Substituting $\\mathbf{f} = \\frac{1}{1 + \\frac{\\nu}{2} \\sigma_a^2} \\mathbf{f}^o$:\n",
    "$ e[k] \\approx \\|\\mathbf{h}^o \\|^2 a[k] + \\mathbf{h}^{\\prime H} \\left( I - \\frac{1}{1 + \\frac{\\nu}{2} \\sigma_a^2} P_{\\mathbf{h}^{\\perp \\prime}} \\right) \\mathbf{v}^{\\prime}[k] $\n",
    "\n",
    "### MMSE Calculation\n",
    "\n",
    "The MMSE is given by:\n",
    "$ \\text{MMSE} = E[|e[k]|^2] $\n",
    "\n",
    "Substituting the error signal:\n",
    "$ \\text{MMSE} = \\|\\mathbf{h}^o \\|^4 \\sigma_a^2 + \\mathbf{h}^{\\prime H} \\left( I - \\frac{1}{1 + \\frac{\\nu}{2} \\sigma_a^2} P_{\\mathbf{h}^{\\perp \\prime}} \\right)^2 \\mathbf{h}^\\prime $\n",
    "\n",
    "### Increase in MSE Due to Channel Estimation Error\n",
    "\n",
    "The increase in MSE can be computed as:\n",
    "$ \\Delta \\text{MSE} = \\text{MMSE} - \\text{MMSE}_{\\text{ideal}} $\n",
    "\n",
    "Where:\n",
    "$ \\text{MMSE}_{\\text{ideal}} = \\|\\mathbf{h}^o \\|^4 \\sigma_a^2 + \\mathbf{h}^{\\prime H} (I - P_{\\mathbf{h}^{\\perp \\prime}})^2 \\mathbf{h}^\\prime $\n",
    "\n",
    "Thus:\n",
    "$ \\Delta \\text{MSE} = (1 - \\alpha)^2 \\mathbf{h}^{\\prime H} P_{\\mathbf{h}^{\\perp \\prime}} \\mathbf{h}^\\prime $\n",
    "\n",
    "For $ R_{\\mathbf{VV}} = \\sigma_v^2 I_m $:\n",
    "$ P_{\\mathbf{h}^{\\perp \\prime}} \\mathbf{h}^\\prime = \\frac{1}{\\sigma_v} P_{\\mathbf{h}^\\perp} \\mathbf{h} = 0 $\n",
    "\n",
    "Therefore, the increase in MSE is zero under this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9866d36-28f4-40a9-8b42-d5ca48a8836e",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x67;)** ___Effect of channel adaptation on LMMSE ICMF operation with short-term IC estimation___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b79eec-07ac-4f35-931c-87d23528024e",
   "metadata": {},
   "source": [
    "In `(f)`, we considered the effect of channel estimation error on the operation of the ICMF when the Interference Canceling (IC) filter $\\mathbf{f}$ is adapted with long-term statistics $R_{d \\mathbf{X}}$, $R_\\mathbf{XX}$. In that case, statistical averaging occurs not only over the noise and the transmitted data but also over the channel estimation error since many instances of this error will be involved and get averaged out. Another possible configuration is shorttime averaging for $\\mathbf{f}$, involving essentially the data between two training symbols, so that the channel estimation error remains constant in such a period. This short-term averaging only averages over noise and transmitted data. The signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter $\\mathbf{h}^{\\perp H}$ will now lead to correlation between $\\mathbf{d}[k]$ and $\\mathbf{x}[k]$.\n",
    "\n",
    "Take again the signal descriptions for $d[k]$ and $\\mathbf{x}[k]$ from `(f)`, up to first order in $\\mathbf{\\tilde{h}}$. Find $R_{d \\mathbf{X}}$ and $R_\\mathbf{XX}$ using averaging over noise and symbols only, up to first order in $\\mathbf{\\tilde{h}}$. Find the LMMSE filter $\\mathbf{f}$ up to first order in $\\mathbf{\\tilde{h}}$, in terms of the unperturbed version $\\mathbf{f}^o$. Note that the perturbation in $\\mathbf{f}$ due to $\\mathbf{\\tilde{h}}$ is proportional to signal power $\\sigma_a^2\\|\\mathbf{h}^o\\|^2$.\n",
    "\n",
    "Express the corresponding error signal $e[k] = d[k] − \\mathbf{f x}[k]$ in terms of the unperturbed $e^o[k]$ and first-order perturbation terms in $\\mathbf{\\tilde{h}}$. Note that the perturbation terms are mutually uncorrelated. Why?\n",
    "\n",
    "Compute the corresponding MMSE, $E \\| e[k] \\|^2$, by now also averaging over $\\mathbf{\\tilde{h}}$, to get a simplified average expression, assuming the LMS adaptation for $\\mathbf{h}[k]$ as in `(e)`.\n",
    "\n",
    "The signal part in $e[k]$ that the receiver will assume on the basis of its knowledge of $\\mathbf{h} = \\mathbf{h}^o − \\mathbf{\\tilde{h}}$ is $\\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ while hence $e[k] − \\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ is considered noise. Compute the resulting SNR with numerator and denominator averaged over $\\mathbf{\\tilde{h}}$ and computed up to first order in $\\nu$. The channel estimation error leads to signal leakage in the bottom branch of the ICMF, which leads to some _signal cancellation_ and ensuing loss in SNR. In the normal Generalized Sidelobe Canceler (GSC), of which the ICMF is a special instance, any error in the blocking filter ($\\mathbf{h}^{\\perp}$ in the ICMF case) leads to signal cancellation that becomes total when the received signal SNR increases (hence the SNR at the output of the GSC goes to zero then). In our analysis the ICMF output SNR remains bounded away from zero due to the fact that as the received SNR increases, the channel estimation error decreases also. Note the similarity with the loss in SNR in `(c)` due to IC filter adaptation by LMS without signal compensation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9546e7b-c773-4a59-a792-aeefe1d664af",
   "metadata": {},
   "source": [
    "To address the problem at hand, let's delve into the specifics of the given scenario and solve each part step-by-step, considering the short-term averaging effects, and the corresponding signal representations.\n",
    "\n",
    "### Signal Descriptions for $d[k]$ and $\\mathbf{x}[k]$\n",
    "\n",
    "From the given information, the signal descriptions are:\n",
    "$ d[k] \\approx \\|\\mathbf{h}^o\\|^2 \\, a[k] + \\mathbf{h}^{o H} \\, \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\, \\mathbf{h}^o \\, a[k] $\n",
    "$ \\mathbf{x}[k] \\approx \\mathbf{h}^{o \\perp H} \\, \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\, \\mathbf{\\tilde{h}} \\, a[k] $\n",
    "\n",
    "Here:\n",
    "- $ \\mathbf{h}^o $ is the true channel vector.\n",
    "- $ \\mathbf{\\tilde{h}} $ is the channel estimation error.\n",
    "- $ a[k] $ is the data symbol.\n",
    "- $ \\mathbf{v}[k] $ is the noise.\n",
    "\n",
    "### Finding $ R_{d \\mathbf{X}} $ and $ R_{\\mathbf{XX}} $\n",
    "\n",
    "#### $ R_{d \\mathbf{X}} $\n",
    "\n",
    "We compute the cross-correlation between $ d[k] $ and $ \\mathbf{x}[k] $:\n",
    "\n",
    "$ R_{d \\mathbf{X}} = E[d[k] \\mathbf{x}^H[k]] $\n",
    "\n",
    "Substituting $ d[k] $ and $ \\mathbf{x}[k] $:\n",
    "\n",
    "$ d[k] \\approx \\|\\mathbf{h}^o\\|^2 a[k] + \\mathbf{h}^{o H} \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] $\n",
    "$ \\mathbf{x}[k] \\approx \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "Since we are averaging over noise $ \\mathbf{v}[k] $ and data $ a[k] $:\n",
    "\n",
    "$ R_{d \\mathbf{X}} = E\\left[ \\left( \\|\\mathbf{h}^o\\|^2 a[k] + \\mathbf{h}^{o H} \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] \\right) \\left( \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] \\right)^H \\right] $\n",
    "\n",
    "Expanding and noting that $ E[a[k]] = 0 $ and $ E[a[k] a^H[k]] = \\sigma_a^2 $, and $ E[\\mathbf{v}[k] \\mathbf{v}^H[k]] = \\mathbf{R_{\\mathbf{VV}}} $:\n",
    "\n",
    "$ R_{d \\mathbf{X}} = \\|\\mathbf{h}^o\\|^2 \\sigma_a^2 \\mathbf{h}^{o \\perp H} + \\mathbf{h}^{o H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} - \\sigma_a^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^o \\mathbf{h}^{o \\perp H} $\n",
    "\n",
    "Since $ \\mathbf{\\tilde{h}} $ is zero-mean and uncorrelated with $\\mathbf{h}^o$:\n",
    "\n",
    "$ R_{d \\mathbf{X}} = \\mathbf{h}^{o H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} + \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} $\n",
    "\n",
    "#### $ R_{\\mathbf{XX}} $\n",
    "\n",
    "We compute the autocorrelation of $ \\mathbf{x}[k] $:\n",
    "\n",
    "$ R_{\\mathbf{XX}} = E[\\mathbf{x}[k] \\mathbf{x}^H[k]] $\n",
    "\n",
    "Substituting $ \\mathbf{x}[k] $:\n",
    "\n",
    "$ \\mathbf{x}[k] \\approx \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$ R_{\\mathbf{XX}} = E\\left[ \\left( \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] \\right) \\left( \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] \\right)^H \\right] $\n",
    "\n",
    "Since $ E[a[k] a^H[k]] = \\sigma_a^2 $ and $ E[\\mathbf{v}[k] \\mathbf{v}^H[k]] = \\mathbf{R_{\\mathbf{VV}}} $:\n",
    "\n",
    "$ R_{\\mathbf{XX}} = \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} + \\sigma_a^2 \\mathbf{h}^{o \\perp H} E[\\mathbf{\\tilde{h}} \\mathbf{\\tilde{h}}^H] \\mathbf{h}^{o \\perp} $\n",
    "\n",
    "Assuming $ E[\\mathbf{\\tilde{h}} \\mathbf{\\tilde{h}}^H] = R_{\\mathbf{\\tilde{h}\\tilde{h}}} $:\n",
    "\n",
    "$ R_{\\mathbf{XX}} = \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} + \\sigma_a^2 \\mathbf{h}^{o \\perp H} R_{\\mathbf{\\tilde{h}\\tilde{h}}} \\mathbf{h}^{o \\perp} $\n",
    "\n",
    "### LMMSE Filter $\\mathbf{f}$\n",
    "\n",
    "The LMMSE filter is given by:\n",
    "$ \\mathbf{f} = R_{d \\mathbf{X}} R_{\\mathbf{XX}}^{-1} $\n",
    "\n",
    "Substituting the derived $ R_{d \\mathbf{X}} $ and $ R_{\\mathbf{XX}} $:\n",
    "\n",
    "$ R_{d \\mathbf{X}} = \\mathbf{h}^{o H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} + \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} $\n",
    "$ R_{\\mathbf{XX}} = \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} + \\sigma_a^2 \\mathbf{h}^{o \\perp H} R_{\\mathbf{\\tilde{h}\\tilde{h}}} \\mathbf{h}^{o \\perp} $\n",
    "\n",
    "Approximating the inverse of $ R_{\\mathbf{XX}} $:\n",
    "\n",
    "$ R_{\\mathbf{XX}}^{-1} \\approx \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1} - \\sigma_a^2 \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1} \\mathbf{h}^{o \\perp H} R_{\\mathbf{\\tilde{h}\\tilde{h}}} \\mathbf{h}^{o \\perp} \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1} $\n",
    "\n",
    "Then:\n",
    "$ \\mathbf{f} \\approx \\mathbf{f}^o + \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1} $\n",
    "\n",
    "### Error Signal $ e[k] $\n",
    "\n",
    "The error signal is:\n",
    "$ e[k] = d[k] - \\mathbf{f}^H \\mathbf{x}[k] $\n",
    "\n",
    "Substituting $ d[k] $ and $ \\mathbf{x}[k] $:\n",
    "\n",
    "$ e[k] = \\|\\mathbf{h}^o\\|^2 a[k] + \\mathbf{h}^{o H} \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] - \\mathbf{f}^H \\left( \\mathbf{h}^{o \\perp H} mathbf{v}[k] + \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] \\right) $\n",
    "\n",
    "Substituting $\\mathbf{f} = \\mathbf{f}^o + \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1}$:\n",
    "\n",
    "$ e[k] = \\|\\mathbf{h}^o\\|^2 a[k] + ( \\mathbf{h}^{o H} - \\mathbf{f}^o \\mathbf{h}^{o \\perp H} ) \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] - \\mathbf{f}^o \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} a[k] $\n",
    "\n",
    "The perturbation term due to the second part of $\\mathbf{f}$:\n",
    "$ - \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} \\left( \\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp} \\right)^{-1} \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] $\n",
    "\n",
    "Thus, the complete error signal is:\n",
    "$ e[k] = \\|\\mathbf{h}^o \\|^2 \\, a[k] + ( \\mathbf{h}^{o H} - \\mathbf{f}^o \\, \\mathbf{h}^{\\perp H} ) \\mathbf{v}[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o \\, a[k] - \\mathbf{f}^o \\, \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}} \\, a[k] - \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\, \\mathbf{h}^{o \\perp} \\, ( \\mathbf{h}^{o \\perp H} \\, \\mathbf{R_{\\mathbf{VV}}} \\, \\mathbf{h}^{o \\perp})^{-1} \\, \\mathbf{h}^{o \\perp H} \\, \\mathbf{v}[k] $\n",
    "\n",
    "### MMSE Calculation\n",
    "\n",
    "To find the MMSE, we compute the expectation of the squared error:\n",
    "\n",
    "$ \\sigma_e^2 = E[|e[k]|^2] $\n",
    "\n",
    "Given the structure of $ e[k] $ and noting the uncorrelated nature of the perturbation terms:\n",
    "\n",
    "$ \\sigma_e^2 = \\sigma_{e^o}^2 + \\frac{\\nu}{2} \\sigma_a^2 \\, \\mathbf{h}^{o H} R_{\\mathbf{VV}} \\, \\mathbf{h}^o + \\frac{\\nu}{2} \\sigma_a^2 \\mathbf{f}^o \\, \\mathbf{h}^{o \\perp H} \\, R_{\\mathbf{VV}} \\, \\mathbf{h}^{o \\perp} \\, \\mathbf{f}^{o H} $\n",
    "\n",
    "The third perturbation term:\n",
    "$ \\sigma_a^4 \\|\\mathbf{h}^o\\|^4 \\, \\mathrm{tr} \\left\\{ \\left( \\mathbf{h}^{o \\perp H} \\, R_{\\mathbf{VV}} \\, \\mathbf{h}^{o \\perp} \\right)^{-1} \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] \\mathbf{v}^H[k] \\mathbf{h}^{o \\perp} \\left( \\mathbf{h}^{o \\perp H} \\, \\mathbf{R_{\\mathbf{VV}}} \\, \\mathbf{h}^{o \\perp} \\right)^{-1} \\mathbf{h}^{o \\perp H} \\mathbf{\\tilde{h}\\tilde{h}}^H \\mathbf{h}^{o\\perp} \\right\\} $\n",
    "\n",
    "Given the uncorrelated nature of the noise and data:\n",
    "\n",
    "$ \\sigma_e^2 = \\sigma_{e^o}^2 + \\frac{\\nu}{2} \\sigma_a^2 \\mathbf{h}^{o H} R_{\\mathbf{VV}} \\mathbf{h}^o + \\frac{\\nu}{2} \\sigma_a^2 \\mathbf{f}^o \\mathbf{h}^{o \\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{o \\perp} \\mathbf{f}^{o H} + \\frac{\\nu}{2} \\sigma_a^4 \\|\\mathbf{h}^o\\|^4 \\left( m - 1 \\right) $\n",
    "\n",
    "### Signal-to-Noise Ratio (SNR)\n",
    "\n",
    "The signal part assumed by the receiver:\n",
    "\n",
    "$ \\|\\mathbf{h}^o - \\mathbf{\\tilde{h}}\\|^2 a[k] \\approx \\|\\mathbf{h}^o\\|^2 a[k] - \\mathbf{h}^{o H} \\mathbf{\\tilde{h}} a[k] - \\mathbf{\\tilde{h}}^H \\mathbf{h}^o a[k] $\n",
    "\n",
    "The noise part:\n",
    "\n",
    "$ (\\mathbf{h}^{o H} - \\mathbf{f}^o \\mathbf{h}^{o \\perp H}) (\\mathbf{v}[k] + \\mathbf{\\tilde{h}} a[k]) - \\sigma_a^2 \\|\\mathbf{h}^o\\|^2 \\mathbf{\\tilde{h}}^H \\mathbf{h}^{o \\perp} (\\mathbf{h}^{o \\perp H} \\mathbf{R_{\\mathbf{VV}}} \\mathbf{h}^{o \\perp})^{-1} \\mathbf{h}^{o \\perp H} \\mathbf{v}[k] $\n",
    "\n",
    "Thus, the ICMF output SNR is:\n",
    "\n",
    "$ \\text{SNR}^{\\text{out}} = \\frac{\\|\\mathbf{h}^o \\|^4 \\sigma_a^2 + \\nu \\sigma_a^2 \\| \\mathbf{h}^\\prime \\|^2}{ \\| P_{\\mathbf{h}^{\\perp \\prime} }^{\\perp} \\mathbf{h}' \\|^2 (1 + \\frac{\\nu}{2} \\sigma_a^2) + \\|\\mathbf{h}^o\\|^4 \\sigma_a^2 \\frac{\\nu}{2} \\sigma_a^2 (m - 1)} $\n",
    "\n",
    "When $ \\text{SNR}^{\\text{Rx}} \\to \\infty $:\n",
    "\n",
    "$ \\text{SNR}^{\\text{out}} \\approx \\frac{1}{\\frac{\\nu}{2} \\sigma_a^2 (m - 1)} $\n",
    "\n",
    "### Explanation\n",
    "\n",
    "The analysis shows that the effect of the channel estimation error on the ICMF leads to signal leakage in the output, impacting the correlation between $ d[k] $ and $ \\mathbf{x}[k] $. The error terms contribute to the MMSE and degrade the performance, yet the bounded nature of the output SNR is maintained due to the decreasing channel estimation error with increasing received SNR. This effect is similar to the loss in SNR observed with LMS adaptation without signal compensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cf5b8-ebfd-4602-9f44-1668487b0bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
