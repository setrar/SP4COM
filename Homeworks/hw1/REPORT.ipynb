{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77296721-a912-4744-a169-8cf8e79c7f17",
   "metadata": {},
   "source": [
    "# &#x1F4DD; REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8233d69-be14-4dd6-85cf-9777966fa064",
   "metadata": {},
   "source": [
    "# Homework &#x0031;&#xFE0F;&#x20E3; \n",
    "\n",
    "Homework policy: the homework is individual. Students are encouraged to discuss with fellow students to try to find the main structure of the solution for a problem, especially if they are totally stuck at the beginning of the problem. However, they should work out the details themselves and write down in their own words only what they understand themselves. For every answer you provide, try to give it in its simplest form, while answering correctly. Results that are available in the course notes can be used and referenced and do not need to be rederived.\n",
    "\n",
    "You can answer in French or in English. Do not forget to answer all subquestions. Word processing (Word, Latex,...) would be appreciated, or scanned readable handwritten notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee1a8b-4ed3-4612-8ac7-53262b62978a",
   "metadata": {},
   "source": [
    "#### ___Spatial Processing: Linear Interference Cancellation___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e9426-5532-4605-abe9-0dbd74d885f7",
   "metadata": {},
   "source": [
    "### **&#x2488;** Adaptation of the Spatial ICMF via LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa06fad-a6b5-4287-82cf-af4fbc1a81c8",
   "metadata": {},
   "source": [
    "<img src=images/ICMF_via_LMS.png width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c408472-99f7-4ab4-b4d0-2a6af597e222",
   "metadata": {},
   "source": [
    "Consider adaptation of the spatial Interference Canceling Matched Filter (ICMF) depicted in the figure above. The received signal $y[k]$ contains m subchannels and the interference canceling filter $\\mathbf{f}$ is represented as a row vector. For a generic value of $\\mathbf{f}$, we get the error signal \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k](\\mathbf{f}) = d[k]− \\mathbf{f} \\; \\mathbf{x}[k] . \\qquad\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For the adaptation of $\\mathbf{f}$, the error signal is $\\epsilon[k]$ which also provides an estimate of the transmitted symbol sequence $a[k]$ (up to a scale factor $\\|\\mathbf{h}\\|^2$), the desired response signal is $d[k]$, which is the spatial matched filter output $\\mathbf{y}_1[k]$, and the input signal is $\\mathbf{x}[k]$, which is also the output $\\mathbf{y}_2[k]$ of the orthogonal complement filter $\\mathbf{h}^{\\perp H}$. The transmitted symbol sequence $a[k]$ and the additive noise sequence $\\mathbf{v}[k]$ are both considered to be temporally white and mutually independent, whereas the noise is spatially colored with covariance matrix $R_\\mathbf{VV}$ (the noise could contain interference). The transmitted power is $\\sigma_a^2$. We assume in a first instance that $\\hat{\\mathbf{h}}[k] = \\mathbf{h}$ (and hence $\\hat{\\mathbf{h}}^\\perp[k] = \\mathbf{h}^\\perp$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a07c8-493f-4581-a5a0-9ae15cb1816d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x61;)** ___LMMSE design___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc23a9-c019-42b3-a560-92b77157b68d",
   "metadata": {},
   "source": [
    "Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$.\n",
    "Let $e[k] = \\epsilon[k](\\mathbf{f}^o)$ be the optimal error signal. Derive an expression for $e[k]$ in terms of the quantities in the figure.\n",
    "\n",
    "Introduce the matrix square root $R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}$ and the transformed quantities\n",
    "$\\mathbf{h}^{'} = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}$. Note that if $R_{\\mathbf{VV}}$ is not a multiple of identity, then $h'$ and $h^{\\perp '}$ are no longer orthogonal. Also introduce $\\mathbf{v}^′[k] = R_{\\mathbf{VV}}^{−1/2}\\mathbf{v}[k]$ for which $R_{\\mathbf{V'V'}} = I_m$ . Find now a simplified expression for $e[k]$ and show that the corresponding MMSE is\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_e^2 = \\sigma_a^2 \\| \\mathbf{h} \\|^4 + \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2\n",
    "\\end{equation}\n",
    "$$\n",
    "where $P_{\\mathbf{g}} = \\mathbf{g}(\\mathbf{g}^H \\mathbf{g})^{−1}\\mathbf{g}^H , P_{\\mathbf{g}}^{\\perp} = I − P_{\\mathbf{g}}$ are the projection matrices on the column space of $\\mathbf{g}$ and its orthogonal complement respectively. When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81885a-cebb-4f3e-8591-6f7a3752248a",
   "metadata": {},
   "source": [
    "&#x1F518; Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304bdc1-b042-4bd2-8f82-cb3fe2ae392f",
   "metadata": {},
   "source": [
    "The Linear Minimum Mean Square Error (LMMSE) filter aims to minimize the mean square error between the actual transmitted symbols and the estimates obtained through the filter. The optimization problem can be stated as finding the filter $\\mathbf{f}^o$ that minimizes the expected value of the square of the error signal $\\epsilon[k]$. In mathematical terms, the LMMSE filter is obtained by solving the following optimization problem:\n",
    "\n",
    "$ \\mathbf{f}^o = \\underset{\\mathbf{f}}{\\text{arg min}} \\; E\\left[ \\left( d[k] - \\mathbf{f} \\mathbf{x}[k] \\right)^2 \\right] $\n",
    "\n",
    "Where $E[\\cdot]$ denotes the expectation operator.\n",
    "\n",
    "This filter takes into account the characteristics of both the signal and the noise to minimize the mean square error in estimating $d[k]$.\n",
    "Let's go through a detailed derivation of the Linear Minimum Mean Square Error (LMMSE) filter $\\mathbf{f}^o$ for the given system, using the ***spatial properties*** of the ***channel vectors*** $\\mathbf{h}$ and $\\mathbf{h}^\\perp$ and the noise covariance matrix $R_{\\mathbf{VV}}$.\n",
    "\n",
    "- ##### Step 1: Establishing Variables and Covariance Matrices\n",
    "\n",
    "___Received signal model___: The received signal $ \\mathbf{y}[k] $ is composed of the transmitted signal affected by the channel and the noise, expressed as:\n",
    "$ \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k] $\n",
    "\n",
    "___Desired response $d[k]$___: The output from the matched filter $ \\mathbf{h}^H $, which ideally extracts the component of the received signal in the direction of $\\mathbf{h}$:\n",
    "$ d[k] = \\mathbf{h}^H \\mathbf{y}_1[k] $\n",
    "\n",
    "___Input signal $\\mathbf{x}[k]$___: The output from the orthogonal complement filter $ \\mathbf{h}^{\\perp H} $, which extracts the component of the received signal orthogonal to $\\mathbf{h}$:\n",
    "$ \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}_2[k] $\n",
    "\n",
    "- ##### Step 2: Calculating Covariance and Cross-Covariance\n",
    "\n",
    "___Auto-covariance $ R_{\\mathbf{xx}} $___ of $\\mathbf{x}[k]$:\n",
    "$ R_{\\mathbf{xx}} = E[\\mathbf{x}[k] \\mathbf{x}[k]^H] = \\mathbf{h}^{\\perp H} E[\\mathbf{y}_2[k] \\mathbf{y}_2[k]^H] \\mathbf{h}^\\perp $\n",
    "$ R_{\\mathbf{xx}} = \\mathbf{h}^{\\perp H} (\\sigma_a^2 \\mathbf{h} \\mathbf{h}^H + R_{\\mathbf{VV}}) \\mathbf{h}^\\perp $ so: $ R_{\\mathbf{xx}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp $\n",
    "\n",
    "&#x1F4A1; ___Projection onto orthogonal complement___: Since $\\mathbf{h}$ and $\\mathbf{h}^\\perp$ are orthogonal, $\\mathbf{h}^{\\perp H} \\mathbf{h} = 0$, [&#x1F4C8; &#x1F449; Graphical Explanation:](_support/projection_onto_orthogonal_complement.ipynb)\n",
    "\n",
    "___Cross-covariance $ R_{d\\mathbf{x}} $___ between $d[k]$ and $\\mathbf{x}[k]$:\n",
    "$ R_{d\\mathbf{x}} = E[d[k] \\mathbf{x}[k]^H] = \\mathbf{h}^H E[\\mathbf{y}_1[k] \\mathbf{y}_2[k]^H] \\mathbf{h}^\\perp $\n",
    "$ R_{d\\mathbf{x}} = \\mathbf{h}^H (\\sigma_a^2 \\mathbf{h} \\mathbf{h}^H + R_{\\mathbf{VV}}) \\mathbf{h}^\\perp $\n",
    ", we have: $ R_{d\\mathbf{x}} = \\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp $\n",
    "\n",
    "&#x1F4A1; Again, using the orthogonality of $\\mathbf{h}$ and $\\mathbf{h}^\\perp$\n",
    "\n",
    "- ##### Step 3: Formulating the LMMSE Filter\n",
    "\n",
    "The LMMSE filter $\\mathbf{f}^o$ that minimizes the mean squared error is given by:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$   \\mathbf{f}^o = R_{d\\mathbf{x}} R_{\\mathbf{xx}}^{-1} $\n",
    "Substituting the derived expressions:\n",
    "$ \n",
    "    \\mathbf{f}^o = (\\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp) (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \n",
    "$\n",
    "\n",
    "This derivation hinges on the orthogonality of $\\mathbf{h}$ and $\\mathbf{h}^\\perp$ and the properties of the noise covariance matrix $R_{\\mathbf{VV}}$. This optimal filter effectively cancels out the interference while preserving the integrity of the desired signal $d[k]$ derived from the received signal $\\mathbf{y}[k]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d24036-c37e-4d16-8295-cc0dd07d472e",
   "metadata": {},
   "source": [
    "&#x1F518; Derive an expression for $e[k]$ in terms of the quantities in the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9ba52-27fc-4e27-bdde-d7ae9a18df29",
   "metadata": {},
   "source": [
    "Given the transformation involving the matrix square root of the noise covariance matrix $ R_{\\mathbf{VV}} $ and its relation to the transformed quantities, we can derive an updated expression for $ e[k] $ in this new transformed space. The aim is to simplify and normalize the relationships by transforming the channel vector and noise vector such that the new noise vector $ \\mathbf{v}'[k] $ has a covariance matrix of $ I_m $, the identity matrix.\n",
    "\n",
    "- #### Transformations Introduced:\n",
    "\n",
    "1. **Noise Covariance Square Root**:\n",
    "   $\n",
    "   R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}\n",
    "   $\n",
    "2. **Transformed Channel Vector**:\n",
    "   $\n",
    "   \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}\n",
    "   $\n",
    "3. **Transformed Noise Vector**:\n",
    "   $\n",
    "   \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2}\\mathbf{v}[k]\n",
    "   $\n",
    "   where $ R_{\\mathbf{V'V'}} = I_m $.\n",
    "\n",
    "- #### Step 1: Redefine the Projections and Signal Components\n",
    "\n",
    "Under these transformations, the relationship between $ d[k] $, $ \\mathbf{x}[k] $, and the projections of vectors changes:\n",
    "- **Received Signal** $ \\mathbf{y}[k] $:\n",
    "  $\n",
    "  \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k] = R_{\\mathbf{VV}}^{-1/2} (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Desired Signal** $ d[k] $:\n",
    "  $\n",
    "  d[k] = \\mathbf{h}^H \\mathbf{y}[k] = (\\mathbf{h}')^H R_{\\mathbf{VV}}^{-1/2} \\mathbf{y}[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Input Signal** $ \\mathbf{x}[k] $:\n",
    "  $\n",
    "  \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}[k]\n",
    "  $\n",
    "  We need to define $ \\mathbf{h}^{\\perp '} $ such that it is orthogonal in the transformed space, which requires additional analysis or assumptions since $ \\mathbf{h}' $ and $ \\mathbf{h}^{\\perp '} $ are not necessarily orthogonal due to the non-identity $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "- #### Step 2: Expression for $ e[k] $ in the Transformed Space\n",
    "\n",
    "Assuming we have derived or defined an appropriate $ \\mathbf{h}^{\\perp '} $:\n",
    "$ \\mathbf{x}[k] = (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "$ e[k] = d[k] - \\mathbf{f}^o \\mathbf{x}[k] $\n",
    "where $ \\mathbf{f}^o $ needs to be recalculated for the transformed space:\n",
    "$ \\mathbf{f}^o = (\\mathbf{h}'^H \\mathbf{h}^{\\perp '}) ((\\mathbf{h}^{\\perp '})^H \\mathbf{h}^{\\perp '})^{-1} $\n",
    "\n",
    "Then:\n",
    "$ e[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) - \\mathbf{f}^o (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "This describes the error in terms of the transformed channel $ \\mathbf{h}' $ and noise $ \\mathbf{v}' $, where:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "- #### Step 3: Rewriting $ e[k] $ in Original Terms\n",
    "\n",
    "To rewrite $ e[k] $ using the original $ \\mathbf{h} $ and $ \\mathbf{v} $, we need to express the transformed variables back in terms of the originals:\n",
    "\n",
    "- ##### a). **Transform Back the Noise and Channel Vectors**\n",
    "- **Transformed Channel**:\n",
    "\n",
    "  $\n",
    "  \\mathbf{h}' a[k] = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} a[k]\n",
    "  $\n",
    "\n",
    "  Multiplied through the error signal expression, this becomes:\n",
    "  $\n",
    "  (\\mathbf{h}')^H \\mathbf{h}' a[k] = \\mathbf{h}^H R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} \\mathbf{h} a[k] = \\mathbf{h}^H \\mathbf{h} a[k]\n",
    "  $\n",
    "  showing that multiplication with the square root and its Hermitian transpose returns to the original form because $ R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} = R_{\\mathbf{VV}} $ and $ \\mathbf{h} $ is normalized or adjusted accordingly.\n",
    "\n",
    "- **Transformed Noise**:\n",
    "\n",
    "  $\n",
    "  \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "  \n",
    "  Using the projection operator:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}} \\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k]\n",
    "  $\n",
    "  This matrix is essentially what happens when we multiply the noise by the orthogonal projection matrix adjusted by the covariance matrix $ R_{\\mathbf{VV}} $. It projects $ \\mathbf{v}[k] $ onto the space orthogonal to $ \\mathbf{h}^{\\perp} $ under the metric induced by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "- ##### b). **Combine Back into the Error Equation**\n",
    "\n",
    "  After substituting these transformations back and simplifying:\n",
    "\n",
    "  $   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "  $e[k] = \\mathbf{h}^H (\\mathbf{h} a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])$\n",
    "\n",
    "  This result shows the a priori error, which accounts for the noise's coloring through $ R_{\\mathbf{VV}} $ and  effectively reduces the noise components aligned with the space spanned by $ \\mathbf{h}^\\perp $ under this coloring. The transformation retains the adaptive filter's capability to handle spatially colored noise and interference correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242acd6b-a0f9-4dbf-bb82-cca03d7d65de",
   "metadata": {},
   "source": [
    "&#x1F518; Finding a simplified expression for $e[k]$ and showing the corresponding MMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f56dd-d1d1-4f1a-8689-deeb4a4cf796",
   "metadata": {},
   "source": [
    "To transition from the complex expression for $ e[k] $ involving the projection matrix on $ \\mathbf{h}^\\perp $ in the original space to a simplified expression involving transformations and projection matrices in the transformed space, we need to map each term appropriately and understand how the transformations and projections relate. Let’s break down this transition step-by-step:\n",
    "\n",
    "___Starting Point___\n",
    "\n",
    "The initial formula:\n",
    "$ \n",
    "e[k] = \\mathbf{h}^H (\\mathbf{h} a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])\n",
    "$\n",
    "\n",
    "This formula accounts for:\n",
    "1. **Signal Component:** $ \\mathbf{h}^H \\mathbf{h} a[k] $\n",
    "2. **Noise Component:** Modified by a projection that reduces noise components along the space spanned by $ \\mathbf{h}^\\perp $, adjusted by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "___Transforming the Terms___\n",
    "\n",
    "1. **Signal Component**\n",
    "\n",
    "- The signal component $\\mathbf{h}^H \\mathbf{h} a[k]$ is straightforward. It simplifies to $\\|\\mathbf{h}\\|^2 a[k]$ since $\\mathbf{h}^H \\mathbf{h}$ is the power of $\\mathbf{h}$, or the square of its norm.\n",
    "\n",
    "2. **Noise Component**\n",
    "\n",
    "- The original noise term uses a projection to eliminate components of noise in the subspace spanned by $ \\mathbf{h}^\\perp $ adjusted by $ R_{\\mathbf{VV}} $. To transform this into the desired format:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k] \n",
    "  $\n",
    "  This term can be seen as a projection of $ \\mathbf{v}[k] $ onto the orthogonal complement of $ \\mathbf{h}^\\perp $ in the metric of $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "To connect this with the transformed version $ \\mathbf{h}' $ and $ \\mathbf{v}' $, recall:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "Thus, rewriting the projection matrix for the transformed variables:\n",
    "- We use $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} = I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H} $ where $ \\mathbf{h}^{\\perp '} = R_{\\mathbf{VV}}^{H/2} \\mathbf{h}^\\perp $.\n",
    "- Applying this to $ \\mathbf{v}'[k] $ gives:\n",
    "  $\n",
    "  \\mathbf{h}'^H \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k] = \\mathbf{h}'^H \\Big[I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H}\\Big] R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "\n",
    "___Conclusion___\n",
    "\n",
    "This translates the projection and transformation of the noise vector into the desired expression:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$\n",
    "e[k] = \\|\\mathbf{h}\\|^2 a[k] + \\mathbf{h}^{'H} \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k]\n",
    "$\n",
    "\n",
    "This simplified formula connects the original space properties $ \\mathbf{h} $, $ \\mathbf{v}[k] $ to their transformed versions under $ R_{\\mathbf{VV}} $, reflecting how signal processing can adaptively minimize noise effects while maintaining focus on the desired signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9408b-0e30-4e0a-9bf2-23816b8d0fa5",
   "metadata": {},
   "source": [
    "&#x1F518; When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03c444-786e-4dbe-95d8-aacaeead3589",
   "metadata": {},
   "source": [
    "To clarify and simplify the explanation regarding the expression $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ when $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, let's focus on a straightforward reformulation:\n",
    "\n",
    "### Background\n",
    "Given the original transformations and projection matrices:\n",
    "- **Channel Transformation**: $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $\n",
    "- **Projection Matrices**: For a vector $ \\mathbf{g} $,\n",
    "  - $ \\mathit{P}_{\\mathbf{g}} = \\mathbf{g} (\\mathbf{g}^H \\mathbf{g})^{-1} \\mathbf{g}^H $\n",
    "  - $ \\mathit{P}_{\\mathbf{g}}^{\\perp} = I - \\mathit{P}_{\\mathbf{g}} $\n",
    "\n",
    "### Key Calculation\n",
    "- The projection $ \\mathit{P}_{\\mathbf{h}}^{\\perp} $ projects onto the space orthogonal to $ \\mathbf{h} $.\n",
    "- Applying this to $ \\mathbf{h}' $, we find that:\n",
    "  $ \\mathit{P}_{\\mathbf{h}}^{\\perp} \\mathbf{h}' = \\mathit{P}_{\\mathbf{h}}^{\\perp} (\\sigma_v \\mathbf{h}) = 0 $\n",
    "  because $ \\mathbf{h}' $ is a scalar multiple of $ \\mathbf{h} $ and lies entirely within the space spanned by $ \\mathbf{h} $.\n",
    "\n",
    "### Simplification\n",
    "Given the transformation $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ actually does not modify $ \\mathbf{h}' $ because it's directly aligned with $ \\mathbf{h} $. Thus, any projection onto $ \\mathbf{h} $ or away from $ \\mathbf{h} $ will not change $ \\mathbf{h}' $:\n",
    "\n",
    "- The expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ effectively simplifies to:\n",
    "  $ \\| \\sigma_v \\mathbf{h} \\|^2 $\n",
    "  by recognizing that $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' $ is simply $ \\mathbf{h}' $ itself (no component is removed by the projection):\n",
    "  $ \\| \\sigma_v \\mathbf{h} \\|^2 = \\sigma_v^2 \\| \\mathbf{h} \\|^2 $\n",
    "\n",
    "### Conclusion\n",
    "This simplification highlights that when $ R_{\\mathbf{VV}} = \\sigma_v^2 I_m $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ retains the full transformed channel vector $ \\mathbf{h}' $ because the transformation does not introduce any components orthogonal to $ \\mathbf{h} $ itself. Therefore, the expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ evaluates to $ \\sigma_v^2 \\| \\mathbf{h} \\|^2 $, representing the power of the channel in the presence of isotropic noise scaled by $ \\sigma_v^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd51b1-ba26-4b78-aafb-d7816621fc3d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x62;)** ___LMS adaptation of $f$___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca82880-fbd8-4b5c-9376-4c8962122c94",
   "metadata": {},
   "source": [
    "The LMS algorithm consists of applying one iteration, per sampling period, of the steepest- descent strategy to the instantaneous error criterion $|\\epsilon[k](\\mathbf{f})|^2 = \\epsilon^*[k](\\mathbf{f}) \\, \\epsilon[k](\\mathbf{f})$. In the complex signals case, this becomes\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{f}[k] = \\left.\\mathbf{f}[k - 1] - \\mu \\frac{\\partial|\\epsilon[k](\\mathbf{f})|^2}{\\partial{\\mathbf{f^*}}}\\right|_{\\mathbf{f}=\\mathbf{f}[k-1]}\n",
    "\\end{equation}\n",
    "$$\n",
    "Work out the gradient term in this LMS update. We shall simplify the notation for the a priori error signal as $\\epsilon[k] = \\epsilon[k](\\mathbf{f}[k - 1])$.\n",
    "To check that the gradient has indeed to be taken w.r.t. $\\mathbf{f}^∗$ (and not $\\mathbf{f}$), express the a posteriori error signal $\\epsilon[k](\\mathbf{f}[k])$ as a function of the a priori error signal and observe that the update leads to a smaller error signal for a proper choice of stepsize $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780709f2-5917-4297-add3-c7b952f55153",
   "metadata": {},
   "source": [
    "&#x1F518; Derive the gradient term in the LMS (Least Mean Squares) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5111f9-0375-4a86-bab3-4d9613e29fe5",
   "metadata": {},
   "source": [
    "To derive the gradient term in the LMS (Least Mean Squares) algorithm update equation for the complex signal case, we start with the given expression for the a priori error signal $\\epsilon[k]$, and how it's used in the adaptation of the filter coefficients:\n",
    "\n",
    "- #####  1. A Priori Error Signal\n",
    "The a priori error signal when the filter $\\mathbf{f}$ is at $\\mathbf{f}[k-1]$ can be expressed as:\n",
    "$\n",
    "\\epsilon[k] = d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k]\n",
    "$\n",
    "where $d[k]$ is the desired signal and $\\mathbf{x}[k]$ is the input signal vector at time $k$.\n",
    "\n",
    "- #####  2. LMS Update Equation\n",
    "The LMS algorithm adjusts the filter coefficients by moving in the direction of the negative gradient of the square of the magnitude of the error signal $|\\epsilon[k]|^2$. The update rule is:\n",
    "$\n",
    "\\mathbf{f}[k] = \\mathbf{f}[k-1] - \\mu \\frac{\\partial |\\epsilon[k]|^2}{\\partial \\mathbf{f}^*}\n",
    "$\n",
    "Here, $|\\epsilon[k]|^2 = \\epsilon^*[k] \\epsilon[k]$ is the squared magnitude of the error, and $\\mu$ is the step size or learning rate.\n",
    "\n",
    "- #####  3. Gradient Calculation\n",
    "To find the gradient of $|\\epsilon[k]|^2$ with respect to $\\mathbf{f}^*$, let's use the definition of the error: $|\\epsilon[k]|^2 = \\epsilon^*[k] \\epsilon[k] = (d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])^* (d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])$\n",
    "\n",
    "Expanding this and taking the derivative with respect to $\\mathbf{f}^*$, we get:$\\frac{\\partial}{\\partial \\mathbf{f}^*}|\\epsilon[k]|^2 = \\frac{\\partial}{\\partial \\mathbf{f}^*}[(d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])^* (d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])]$\n",
    "\n",
    "Using the product rule and noting that the derivative of a constant (here, $d[k]$) is zero, we simplify to:$\\frac{\\partial}{\\partial \\mathbf{f}^*}|\\epsilon[k]|^2 = -\\mathbf{x}[k] (d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])^*$\n",
    "\n",
    "This simplifies to:$\\frac{\\partial}{\\partial \\mathbf{f}^*}|\\epsilon[k]|^2 = -\\mathbf{x}[k] \\epsilon^*[k]$\n",
    "\n",
    "- #####  4. Inserting the Gradient into the LMS Update\n",
    "Plugging this gradient into the LMS update formula, we get:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$\n",
    "\\mathbf{f}[k] = \\mathbf{f}[k-1] + \\mu \\mathbf{x}[k] \\epsilon^*[k]\n",
    "$\n",
    "\n",
    "This is the standard complex LMS update, which correctly adjusts $\\mathbf{f}[k-1]$ by moving in the direction that minimizes the magnitude of the error signal, provided the step size $\\mu$ is chosen properly to ensure convergence.\n",
    "\n",
    "- #####  5. Gradient w.r.t. $\\mathbf{f}^*$ Justification\n",
    "Taking the gradient with respect to $\\mathbf{f}^*$ instead of $\\mathbf{f}$ is necessary due to the conjugation in the error term $\\epsilon[k]$. In complex-valued signal processing, derivatives with respect to the conjugate of a variable naturally arise due to the presence of conjugate operations in the formulation of power or energy expressions, ensuring that the gradients are correctly computed for complex numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73706076-74b0-47e4-b005-3f4745c42447",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x63;)** ___Steady-state analysis of LMS adaptation of $\\mathbf{f}$___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210e57-0550-442c-b96c-8cbd624a8e12",
   "metadata": {},
   "source": [
    "Let $\\mathbf{\\tilde{f}}[k] = \\mathbf{f}^o − \\mathbf{f}[k]$ be the filter error. Note that due to the presumed temporal whiteness of\n",
    "$a[k]$ and $\\mathbf{v}[k]$, also $d[k]$ and $\\mathbf{x}[k]$ are temporally white. Hence $\\mathbf{\\tilde{f}}[k−1]$ and $e[k]$ are independent\n",
    "(strictly speaking only uncorrelated).\n",
    "\n",
    "Let $R_{\\mathbf{\\tilde{f}}\\mathbf{\\tilde{f}}}[k] = \\mathrm{E} \\, \\mathbf{\\tilde{f}}^H[k] \\, \\mathbf{\\tilde{f}}[k]$. We can write for the a priori error signal and MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k] = e[k] + \\mathbf{\\tilde{f}}[k−1] \\mathbf{x}[k] \\implies \\sigma_{\\epsilon[k]}^2 = \\sigma_e^2 +tr\\{R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] \\, R_{\\mathbf{XX}} \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The LMS update (3) for $\\mathbf{f}$ leads to the following recursion for $\\mathbf{\\tilde{f}}[k]$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, \\mathbf{x}[k] \\, \\mathbf{x}^H[k]) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which, using the averaging analysis for small stepsize, can be approximated by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, R_{\\mathbf{XX}}) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "From (6), obtain the time evolution for the filter error correlation matrix $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k]$. Assume\n",
    "$\\mu$ small so that $I − \\mu R_{\\mathbf{xx}}$ is stable and neglect second-order terms in $\\mu$. In steady-state $\\sigma_{\\epsilon[\\infty]}^2 = \\sigma_{\\epsilon}^2$ and $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[\\infty] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}$  . Show now from (6) that we obtain for the steady-state Excess MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "EMSE = \\frac{\\mu}{2} \\, \\sigma_e^2 \\, tr\\{R_{\\mathbf{XX}} \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note that $R_{\\mathbf{XX}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{\\perp}$ is not Toeplitz and its diagonal elements are not all equal. Using the simplified expression derived in `(a)` for $e[k]$, show the corresponding expression for the a priori error signal $\\epsilon[k]$.\n",
    "\n",
    "The signal part in $\\epsilon[k]$ is the term containing $a[k]$ and all the rest is noise. Using the expression for MMSE in `(2)`, derive an expression for the SNR in $\\epsilon[k]$. Note that the noise term contains a signal part which limits the SNR attainable by the adaptive system. This is due to the fact that the signal part in the error signal $e[k]$ acts like noise for the adaptation of the filter $\\mathbf{f}[k]$. This problem is generic for any adaptation algorithm and not just specific for LMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eec7bf-791b-4f4e-854b-2129ced67608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee90f58-beee-419d-887b-688e66cecb98",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x64;)** ___Steady-state analysis of signal compensated LMS adaptation of $\\mathbf{f}$___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f70cf-4f6a-4aa3-a036-13eee94f6522",
   "metadata": {},
   "source": [
    "Consider now compensating the signal part in the desired response signal $d[k]$ for the LMS adaptation. So we shall take as desired response $d[k] = \\mathbf{h}^H (\\mathbf{y}[k] − \\mathbf{h} \\, a[k]) = \\mathbf{h}^H \\, \\mathbf{v}[k]$. The goal of the receiver is to detect the $a[k]$ which are hence unknown. The way this signal compensation can be implemented then is by either limiting the update for f to time instants at which the $a[k]$ are training symbols (used for estimating h also, see further) or by using the detected $a[k]$ (decision-directed (DD) strategy). In the DD strategy, the symbol $a[k]$ gets detected from $\\mathbf{h}^H \\, \\mathbf{y}[k] − \\mathbf{f}[k - 1]$ (or delay needs to be introduced for the updating of f if also channel decoding gets exploited to get more reliable $a[k]$). Making abstraction of these details, consider hence $d[k] = \\mathbf{h}^H \\, \\mathbf{v}[k]$.\n",
    "\n",
    "Does the signal compensation influence the optimal filter setting $\\mathbf{f}^o$? What do the optimal error signal $e[k]$ and associated MMSE $\\sigma_e^2$ become?\n",
    "\n",
    "The signal compensation only gets done for the adaptation of $\\mathbf{f}$. The thus adapted $\\mathbf{f}$ then gets used in the original ICMF circuit. So, at the output of the ICMF, with the adapted $\\mathbf{f}$, what does the SNR become? With the signal compensation, the SNR degradation due to the adaptation of $\\mathbf{f}$ can be made arbitrarily small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad6d78-c0d2-4aba-98a8-5edbd3b8f5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6107eb07-34d6-4669-a68c-00374a4b2742",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x65;)** ___LMS adaptation of $\\mathbf{f}$___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12719ec-7869-4f17-a13a-2bd5d5fc8209",
   "metadata": {},
   "source": [
    "The transmitted symbols are in fact partitioned into known training symbols and actual data symbols. The training symbols get inserted periodically. They get used to adapt the channel estimate. From now on we shall denote the true value of the channel as ho (assumed time-invariant). For the adaptation of the channel estimate, consider the error signal $\\mathbf{w}[k](\\mathbf{h}) = \\mathbf{y}[k] − \\mathbf{h} \\, a[k]$. The optimal value for the error signal $\\mathbf{w}[k](\\mathbf{h}^o)$ has already been specified in the problem formulation. What is it?\n",
    "\n",
    "The LMS algorithm performs one iteration of the steepest-descent strategy per training symbol to the instantaneous error criterion $\\|\\mathbf{w}[k](\\mathbf{h})\\|^2 = \\mathbf{w}^H[k](h) \\, \\mathbf{w}[k](\\mathbf{h})$. Derive the LMS algorithm that updates the channel estimate $\\mathbf{h}[k]$ (which could have been denoted also as $\\mathbf{\\hat{h}}[k]$, but let’s keep $\\mathbf{h}[k]$). Denote the a priori error signal as $\\mathbf{w}[k]$ and the stepsize as $\\nu$. Note that the time index now is no longer the true time index but a counter for the training symbols only, since adaptation occurs only when a symbol is a training symbol.\n",
    "Develop the recursion for the channel estimation error $\\mathbf{\\hat{h}}[k] = \\mathbf{h}^o − \\mathbf{h}[k]$. Find the steady-state value for $R_\\mathbf{\\hat{h}\\hat{h}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520b431-45dc-4421-9830-2eff1a3c813c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e626d72c-7e8c-47c6-aeca-e75f92cf93ce",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x66;)** ___Effect of channel adaptation on LMMSE ICMF operation with long-term IC estimation___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfafe8-4c76-4e2c-a719-48982099f147",
   "metadata": {},
   "source": [
    "Consider now the use of the adapted $\\mathbf{h}[k]$ in the ICMF: for any data symbol $a[k]$, the $\\mathbf{h}$ that will be used is the one adapted with LMS at the latest training symbol before the current data symbol. The main effect is that the channel estimation error $\\mathbf{\\tilde{h}}$ will lead to signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter $\\mathbf{h}^{\\perp H}$. The effect of the error $\\mathbf{\\tilde{h}}$ on $\\mathbf{h}^\\perp$ will depend on the choice of $\\mathbf{h}^\\perp$. Assuming the error to be small, we can perform a first-order analysis of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "0 = \\mathbf{h}^{\\perp H} \\, \\mathbf{h} = (\\mathbf{h}^{o \\perp} − \\mathbf{\\tilde{h}}^{\\perp})^H (\\mathbf{h}^o − \\mathbf{\\tilde{h}}) \\approx \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}} − \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\implies \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\approx - \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\tilde{h}}^{\\perp}$ is not the orthogonal complement of $\\mathbf{\\tilde{h}}$  but the error on $\\mathbf{h}^{\\perp}$.\n",
    "\n",
    "Describe the signals $d[k]$ and $\\mathbf{x}[k]$ for the ICMF operation in terms of $\\mathbf{h}^o$, $\\mathbf{\\tilde{h}}$ and their orthogonal complement versions, and $\\mathbf{a}[k]$ and $\\mathbf{v}[k]$, neglecting products of noise terms, and using `(8)`.\n",
    "\n",
    "Find $R_{d\\mathbf{X}}$ and $R_\\mathbf{XX}$. Find the LMMSE filter $\\mathbf{f}$ in terms of the unperturbed version $\\mathbf{f}^o$.\n",
    "\n",
    "Express the corresponding error signal $e[k]$, and MMSE in terms of $\\mathbf{h}^{'} = R_\\mathbf{VV}^{H/2} \\mathbf{h}^o , \\mathbf{h}^{\\perp '} =\n",
    "R_\\mathbf{VV}^{H/2} \\mathbf{h}^{o \\perp}$. Give the increase in MSE due to the channel estimation error. How much is this increase when $R_\\mathbf{VV} = \\sigma_v^2I_m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a9081-50a8-4d10-bb91-20636a82e4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9866d36-28f4-40a9-8b42-d5ca48a8836e",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x67;)** ___Effect of channel adaptation on LMMSE ICMF operation with short-term IC estimation___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b79eec-07ac-4f35-931c-87d23528024e",
   "metadata": {},
   "source": [
    "In `(f)`, we considered the effect of channel estimation error on the operation of the ICMF when the Interference Canceling (IC) filter $\\mathbf{f}$ is adapted with long-term statistics $R_{d \\mathbf{X}}$, $R_\\mathbf{XX}$. In that case, statistical averaging occurs not only over the noise and the transmitted data but also over the channel estimation error since many instances of this error will be involved and get averaged out. Another possible configuration is shorttime averaging for $\\mathbf{f}$, involving essentially the data between two training symbols, so that the channel estimation error remains constant in such a period. This short-term averaging only averages over noise and transmitted data. The signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter h⊥H will now lead to correlation between $\\mathbf{d}[k]$ and $\\mathbf{x}[k]$.\n",
    "\n",
    "Take again the signal descriptions for $d[k]$ and $\\mathbf{x}[k]$ from `(f)`, up to first order in he. Find $R_{d \\mathbf{X}}$ and $R_\\mathbf{XX}$ using averaging over noise and symbols only, up to first order in he. Find the LMMSE filter $\\mathbf{f}$ up to first order in $\\mathbf{\\tilde{h}}$, in terms of the unperturbed version $\\mathbf{f}^o$. Note that the perturbation in $\\mathbf{f}$ due to $\\mathbf{\\tilde{h}}$ is proportional to signal power $\\sigma_a^2\\|\\mathbf{h}^o\\|^2$.\n",
    "\n",
    "Express the corresponding error signal $e[k] = d[k] − \\mathbf{f x}[k]$ in terms of the unperturbed $e^o[k]$ and first-order perturbation terms in $\\mathbf{\\tilde{h}}$. Note that the perturbation terms are mutually uncorrelated. Why?\n",
    "\n",
    "Compute the corresponding MMSE, $E \\| e[k] \\|^2$, by now also averaging over $\\mathbf{\\tilde{h}}$, to get a simplified average expression, assuming the LMS adaptation for $\\mathbf{h}[k]$ as in `(e)`.\n",
    "\n",
    "The signal part in $e[k]$ that the receiver will assume on the basis of its knowledge of $\\mathbf{h} = \\mathbf{h}^o − \\mathbf{\\tilde{h}}$ is $\\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ while hence $e[k] − \\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ is considered noise. Compute the resulting SNR with numerator and denominator averaged over $\\mathbf{\\tilde{h}}$ and computed up to first order in $\\nu$. The channel estimation error leads to signal leakage in the bottom branch of the ICMF, which leads to some _signal cancellation_ and ensuing loss in SNR. In the normal Generalized Sidelobe Canceler (GSC), of which the ICMF is a special instance, any error in the blocking filter ($\\mathbf{h}^{\\perp}$ in the ICMF case) leads to signal cancellation that becomes total when the received signal SNR increases (hence the SNR at the output of the GSC goes to zero then). In our analysis the ICMF output SNR remains bounded away from zero due to the fact that as the received SNR increases, the channel estimation error decreases also. Note the similarity with the loss in SNR in `(c)` due to IC filter adaptation by LMS without signal compensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8d7de-9633-4c3d-830b-5ada5af4f250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
