{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77296721-a912-4744-a169-8cf8e79c7f17",
   "metadata": {},
   "source": [
    "# &#x1F4DD; REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8233d69-be14-4dd6-85cf-9777966fa064",
   "metadata": {},
   "source": [
    "# Homework &#x0031;&#xFE0F;&#x20E3; \n",
    "\n",
    "Homework policy: the homework is individual. Students are encouraged to discuss with fellow students to try to find the main structure of the solution for a problem, especially if they are totally stuck at the beginning of the problem. However, they should work out the details themselves and write down in their own words only what they understand themselves. For every answer you provide, try to give it in its simplest form, while answering correctly. Results that are available in the course notes can be used and referenced and do not need to be rederived.\n",
    "\n",
    "You can answer in French or in English. Do not forget to answer all subquestions. Word processing (Word, Latex,...) would be appreciated, or scanned readable handwritten notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee1a8b-4ed3-4612-8ac7-53262b62978a",
   "metadata": {},
   "source": [
    "#### ___Spatial Processing: Linear Interference Cancellation___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e9426-5532-4605-abe9-0dbd74d885f7",
   "metadata": {},
   "source": [
    "### **&#x2488;** Adaptation of the Spatial ICMF via LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa06fad-a6b5-4287-82cf-af4fbc1a81c8",
   "metadata": {},
   "source": [
    "<img src=images/ICMF_via_LMS.png width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c408472-99f7-4ab4-b4d0-2a6af597e222",
   "metadata": {},
   "source": [
    "Consider adaptation of the spatial Interference Canceling Matched Filter (ICMF) depicted in the figure above. The received signal $y[k]$ contains m subchannels and the interference canceling filter $\\mathbf{f}$ is represented as a row vector. For a generic value of $\\mathbf{f}$, we get the error signal \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k](\\mathbf{f}) = d[k]− \\mathbf{f} \\; \\mathbf{x}[k] . \\qquad\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For the adaptation of $\\mathbf{f}$, the error signal is $\\epsilon[k]$ which also provides an estimate of the transmitted symbol sequence $a[k]$ (up to a scale factor $\\|\\mathbf{h}\\|^2$), the desired response signal is $d[k]$, which is the spatial matched filter output $\\mathbf{y}_1[k]$, and the input signal is $\\mathbf{x}[k]$, which is also the output $\\mathbf{y}_2[k]$ of the orthogonal complement filter $\\mathbf{h}^{\\perp H}$. The transmitted symbol sequence $a[k]$ and the additive noise sequence $\\mathbf{v}[k]$ are both considered to be temporally white and mutually independent, whereas the noise is spatially colored with covariance matrix $R_\\mathbf{VV}$ (the noise could contain interference). The transmitted power is $\\sigma_a^2$. We assume in a first instance that $\\hat{\\mathbf{h}}[k] = \\mathbf{h}$ (and hence $\\hat{\\mathbf{h}}^\\perp[k] = \\mathbf{h}^\\perp$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a07c8-493f-4581-a5a0-9ae15cb1816d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x61;)** ___LMMSE design___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc23a9-c019-42b3-a560-92b77157b68d",
   "metadata": {},
   "source": [
    "Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$.\n",
    "Let $e[k] = \\epsilon[k](\\mathbf{f}^o)$ be the optimal error signal. Derive an expression for $e[k]$ in terms of the quantities in the figure.\n",
    "\n",
    "Introduce the matrix square root $R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}$ and the transformed quantities\n",
    "$\\mathbf{h}^{'} = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}$. Note that if $R_{\\mathbf{VV}}$ is not a multiple of identity, then $h'$ and $h^{\\perp '}$ are no longer orthogonal. Also introduce $\\mathbf{v}^′[k] = R_{\\mathbf{VV}}^{−1/2}\\mathbf{v}[k]$ for which $R_{\\mathbf{V'V'}} = I_m$ . Find now a simplified expression for $e[k]$ and show that the corresponding MMSE is\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_e^2 = \\sigma_a^2 \\| \\mathbf{h} \\|^4 + \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2\n",
    "\\end{equation}\n",
    "$$\n",
    "where $P_{\\mathbf{g}} = \\mathbf{g}(\\mathbf{g}^H \\mathbf{g})^{−1}\\mathbf{g}^H , P_{\\mathbf{g}}^{\\perp} = I − P_{\\mathbf{g}}$ are the projection matrices on the column space of $\\mathbf{g}$ and its orthogonal complement respectively. When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81885a-cebb-4f3e-8591-6f7a3752248a",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Express the LMMSE filter $\\mathbf{f}^o$, that minimizes $\\sigma_{\\epsilon}^2$, in terms of $\\mathbf{h}$, $\\mathbf{h}^{\\perp}$ and $R_\\mathbf{VV}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eade45-bc3b-4da7-9af6-d47d4888cf09",
   "metadata": {},
   "source": [
    "To derive the Linear Minimum Mean Square Error (LMMSE) filter in a condensed form that efficiently incorporates the spatial properties of channel vectors and the noise covariance matrix, let's follow a straightforward approach:\n",
    "\n",
    "***Overview***\n",
    "\n",
    "The LMMSE filter aims to minimize the expected square error between the desired response and the output of a filter applied to an input signal. This filter accounts for the signal and noise characteristics to optimize signal estimation.\n",
    "\n",
    "In mathematical terms, the LMMSE filter is obtained by solving the following optimization problem: $ \\mathbf{f}^o = \\underset{\\mathbf{f}}{\\text{arg min}} \\; E\\left[ \\left( d[k] - \\mathbf{f} \\mathbf{x}[k] \\right)^2 \\right] $ Where $E[\\cdot]$ denotes the expectation operator.\n",
    "\n",
    "***Derivation Steps***\n",
    "\n",
    "- ##### **Step 1: Define Variables**\n",
    "  \n",
    "- **Received Signal $ \\mathbf{y}[k] $**: Composed of the transmitted signal affected by the channel $ \\mathbf{h} $ and noise $ \\mathbf{v}[k] $.\n",
    "  $\n",
    "  \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k]\n",
    "  $\n",
    "\n",
    "- **Desired Response $ d[k] $**: The output from the matched filter targeting the component in the direction of $ \\mathbf{h} $.\n",
    "  $\n",
    "  d[k] = \\mathbf{h}^H \\mathbf{y}[k]\n",
    "  $\n",
    "\n",
    "- **Input Signal $ \\mathbf{x}[k] $**: Extracts the component orthogonal to $ \\mathbf{h} $.\n",
    "  $\n",
    "  \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}[k]\n",
    "  $\n",
    "\n",
    "- ##### **Step 2: Calculate Covariance**\n",
    "\n",
    "- **Auto-Covariance $ R_{\\mathbf{xx}} $** and **Cross-Covariance $ R_{d\\mathbf{x}} $** based on $ \\mathbf{x}[k] $ and $ d[k] $:\n",
    "  $\n",
    "  R_{\\mathbf{xx}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp, \\quad R_{d\\mathbf{x}} = \\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp\n",
    "  $\n",
    "\n",
    "- ##### **Step 3: LMMSE Filter Formula**\n",
    "  \n",
    "- The filter that minimizes the mean squared error:\n",
    "  $\n",
    "  \\mathbf{f}^o = R_{d\\mathbf{x}} R_{\\mathbf{xx}}^{-1}\n",
    "  $\n",
    "  Substituting the derived covariance expressions, we obtain:\n",
    "\n",
    "  $   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "  $\n",
    "  \\mathbf{f}^o = (\\mathbf{h}^H R_{\\mathbf{VV}} \\mathbf{h}^\\perp) (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1}\n",
    "  $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d24036-c37e-4d16-8295-cc0dd07d472e",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Derive an expression for $e[k]$ in terms of the quantities in the figure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9ba52-27fc-4e27-bdde-d7ae9a18df29",
   "metadata": {},
   "source": [
    "Given the transformation involving the matrix square root of the noise covariance matrix $ R_{\\mathbf{VV}} $ and its relation to the transformed quantities, we can derive an updated expression for $ e[k] $ in this new transformed space. The aim is to simplify and normalize the relationships by transforming the channel vector and noise vector such that the new noise vector $ \\mathbf{v}'[k] $ has a covariance matrix of $ I_m $, the identity matrix.\n",
    "\n",
    "- #### Transformations Introduced:\n",
    "\n",
    "1. **Noise Covariance Square Root**:\n",
    "   $\n",
    "   R_{\\mathbf{VV}} = R_{\\mathbf{VV}}^{1/2}R_{\\mathbf{VV}}^{H/2}\n",
    "   $\n",
    "2. **Transformed Channel Vector**:\n",
    "   $\n",
    "   \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2}\\mathbf{h}\n",
    "   $\n",
    "3. **Transformed Noise Vector**:\n",
    "   $\n",
    "   \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2}\\mathbf{v}[k]\n",
    "   $\n",
    "   where $ R_{\\mathbf{V'V'}} = I_m $.\n",
    "\n",
    "- #### Step 1: Redefine the Projections and Signal Components\n",
    "\n",
    "Under these transformations, the relationship between $ d[k] $, $ \\mathbf{x}[k] $, and the projections of vectors changes:\n",
    "- **Received Signal** $ \\mathbf{y}[k] $:\n",
    "  $\n",
    "  \\mathbf{y}[k] = \\mathbf{h} a[k] + \\mathbf{v}[k] = R_{\\mathbf{VV}}^{-1/2} (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Desired Signal** $ d[k] $:\n",
    "  $\n",
    "  d[k] = \\mathbf{h}^H \\mathbf{y}[k] = (\\mathbf{h}')^H R_{\\mathbf{VV}}^{-1/2} \\mathbf{y}[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k])\n",
    "  $\n",
    "\n",
    "- **Input Signal** $ \\mathbf{x}[k] $:\n",
    "  $\n",
    "  \\mathbf{x}[k] = \\mathbf{h}^{\\perp H} \\mathbf{y}[k]\n",
    "  $\n",
    "  We need to define $ \\mathbf{h}^{\\perp '} $ such that it is orthogonal in the transformed space, which requires additional analysis or assumptions since $ \\mathbf{h}' $ and $ \\mathbf{h}^{\\perp '} $ are not necessarily orthogonal due to the non-identity $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "- #### Step 2: Expression for $ e[k] $ in the Transformed Space\n",
    "\n",
    "Assuming we have derived or defined an appropriate $ \\mathbf{h}^{\\perp '} $:\n",
    "$ \\mathbf{x}[k] = (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "$ e[k] = d[k] - \\mathbf{f}^o \\mathbf{x}[k] $\n",
    "where $ \\mathbf{f}^o $ needs to be recalculated for the transformed space:\n",
    "$ \\mathbf{f}^o = (\\mathbf{h}'^H \\mathbf{h}^{\\perp '}) ((\\mathbf{h}^{\\perp '})^H \\mathbf{h}^{\\perp '})^{-1} $\n",
    "\n",
    "Then:\n",
    "$ e[k] = (\\mathbf{h}')^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) - \\mathbf{f}^o (\\mathbf{h}^{\\perp '})^H (\\mathbf{h}' a[k] + \\mathbf{v}'[k]) $\n",
    "\n",
    "This describes the error in terms of the transformed channel $ \\mathbf{h}' $ and noise $ \\mathbf{v}' $, where:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "- #### Step 3: Rewriting $ e[k] $ in Original Terms\n",
    "\n",
    "To rewrite $ e[k] $ using the original $ \\mathbf{h} $ and $ \\mathbf{v} $, we need to express the transformed variables back in terms of the originals:\n",
    "\n",
    "- ##### a). **Transform Back the Noise and Channel Vectors**\n",
    "- **Transformed Channel**:\n",
    "\n",
    "  $\n",
    "  \\mathbf{h}' a[k] = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} a[k]\n",
    "  $\n",
    "\n",
    "  Multiplied through the error signal expression, this becomes:\n",
    "  $\n",
    "  (\\mathbf{h}')^H \\mathbf{h}' a[k] = \\mathbf{h}^H R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} \\mathbf{h} a[k] = \\mathbf{h}^H \\mathbf{h} a[k]\n",
    "  $\n",
    "  showing that multiplication with the square root and its Hermitian transpose returns to the original form because $ R_{\\mathbf{VV}}^{H/2} R_{\\mathbf{VV}}^{1/2} = R_{\\mathbf{VV}} $ and $ \\mathbf{h} $ is normalized or adjusted accordingly.\n",
    "\n",
    "- **Transformed Noise**:\n",
    "\n",
    "  $\n",
    "  \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "  \n",
    "  Using the projection operator:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}} \\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k]\n",
    "  $\n",
    "  This matrix is essentially what happens when we multiply the noise by the orthogonal projection matrix adjusted by the covariance matrix $ R_{\\mathbf{VV}} $. It projects $ \\mathbf{v}[k] $ onto the space orthogonal to $ \\mathbf{h}^{\\perp} $ under the metric induced by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "- ##### b). **Combine Back into the Error Equation**\n",
    "\n",
    "  After substituting these transformations back and simplifying:\n",
    "\n",
    "  $   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "  $e[k] = \\mathbf{h}^H (\\mathbf{h} a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])$\n",
    "\n",
    "  This result shows the a priori error, which accounts for the noise's coloring through $ R_{\\mathbf{VV}} $ and  effectively reduces the noise components aligned with the space spanned by $ \\mathbf{h}^\\perp $ under this coloring. The transformation retains the adaptive filter's capability to handle spatially colored noise and interference correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242acd6b-a0f9-4dbf-bb82-cca03d7d65de",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Finding a simplified expression for $e[k]$ and showing the corresponding MMSE\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f56dd-d1d1-4f1a-8689-deeb4a4cf796",
   "metadata": {},
   "source": [
    "To transition from the complex expression for $ e[k] $ involving the projection matrix on $ \\mathbf{h}^\\perp $ in the original space to a simplified expression involving transformations and projection matrices in the transformed space, we need to map each term appropriately and understand how the transformations and projections relate. Let’s break down this transition step-by-step:\n",
    "\n",
    "___Starting Point___\n",
    "\n",
    "The initial formula:\n",
    "$ \n",
    "e[k] = \\mathbf{h}^H (\\mathbf{h} a[k] + \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k])\n",
    "$\n",
    "\n",
    "This formula accounts for:\n",
    "1. **Signal Component:** $ \\mathbf{h}^H \\mathbf{h} a[k] $\n",
    "2. **Noise Component:** Modified by a projection that reduces noise components along the space spanned by $ \\mathbf{h}^\\perp $, adjusted by $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "___Transforming the Terms___\n",
    "\n",
    "1. **Signal Component**\n",
    "\n",
    "- The signal component $\\mathbf{h}^H \\mathbf{h} a[k]$ is straightforward. It simplifies to $\\|\\mathbf{h}\\|^2 a[k]$ since $\\mathbf{h}^H \\mathbf{h}$ is the power of $\\mathbf{h}$, or the square of its norm.\n",
    "\n",
    "2. **Noise Component**\n",
    "\n",
    "- The original noise term uses a projection to eliminate components of noise in the subspace spanned by $ \\mathbf{h}^\\perp $ adjusted by $ R_{\\mathbf{VV}} $. To transform this into the desired format:\n",
    "  $\n",
    "  \\Big[ \\mathit{I_m} - R_{\\mathbf{VV}}\\mathbf{h}^{\\perp} (\\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^\\perp)^{-1} \\mathbf{h}^{\\perp H} \\Big] \\mathbf{v}[k] \n",
    "  $\n",
    "  This term can be seen as a projection of $ \\mathbf{v}[k] $ onto the orthogonal complement of $ \\mathbf{h}^\\perp $ in the metric of $ R_{\\mathbf{VV}} $.\n",
    "\n",
    "To connect this with the transformed version $ \\mathbf{h}' $ and $ \\mathbf{v}' $, recall:\n",
    "- $ \\mathbf{h}' = R_{\\mathbf{VV}}^{H/2} \\mathbf{h} $\n",
    "- $ \\mathbf{v}'[k] = R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k] $\n",
    "\n",
    "Thus, rewriting the projection matrix for the transformed variables:\n",
    "- We use $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} = I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H} $ where $ \\mathbf{h}^{\\perp '} = R_{\\mathbf{VV}}^{H/2} \\mathbf{h}^\\perp $.\n",
    "- Applying this to $ \\mathbf{v}'[k] $ gives:\n",
    "  $\n",
    "  \\mathbf{h}'^H \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k] = \\mathbf{h}'^H \\Big[I - \\mathbf{h}^{\\perp '} (\\mathbf{h}^{\\perp 'H} \\mathbf{h}^{\\perp '})^{-1} \\mathbf{h}^{\\perp 'H}\\Big] R_{\\mathbf{VV}}^{-1/2} \\mathbf{v}[k]\n",
    "  $\n",
    "\n",
    "___Conclusion___\n",
    "\n",
    "This translates the projection and transformation of the noise vector into the following expression:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$\n",
    "e[k] = \\|\\mathbf{h}\\|^2 a[k] + \\mathbf{h}^{'H} \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{v}'[k]\n",
    "$\n",
    "\n",
    "This simplified formula connects the original space properties $ \\mathbf{h} $, $ \\mathbf{v}[k] $ to their transformed versions under $ R_{\\mathbf{VV}} $, reflecting how signal processing can adaptively minimize noise effects while maintaining focus on the desired signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9408b-0e30-4e0a-9bf2-23816b8d0fa5",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** When $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, what does $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ simplify to?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03c444-786e-4dbe-95d8-aacaeead3589",
   "metadata": {},
   "source": [
    "To clarify and simplify the explanation regarding the expression $\\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2$ when $R_{\\mathbf{VV}} = \\sigma_v^2 I_m$, let's focus on a straightforward reformulation:\n",
    "\n",
    "___Background___\n",
    "\n",
    "Given the original transformations and projection matrices:\n",
    "- **Channel Transformation**: $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $\n",
    "- **Projection Matrices**: For a vector $ \\mathbf{g} $,\n",
    "  - $ \\mathit{P}_{\\mathbf{g}} = \\mathbf{g} (\\mathbf{g}^H \\mathbf{g})^{-1} \\mathbf{g}^H $\n",
    "  - $ \\mathit{P}_{\\mathbf{g}}^{\\perp} = I - \\mathit{P}_{\\mathbf{g}} $\n",
    "\n",
    "___Key Calculation___\n",
    "\n",
    "- The projection $ \\mathit{P}_{\\mathbf{h}}^{\\perp} $ projects onto the space orthogonal to $ \\mathbf{h} $.\n",
    "- Applying this to $ \\mathbf{h}' $, we find that:\n",
    "  $ \\mathit{P}_{\\mathbf{h}}^{\\perp} \\mathbf{h}' = \\mathit{P}_{\\mathbf{h}}^{\\perp} (\\sigma_v \\mathbf{h}) = 0 $\n",
    "  because $ \\mathbf{h}' $ is a scalar multiple of $ \\mathbf{h} $ and lies entirely within the space spanned by $ \\mathbf{h} $.\n",
    "\n",
    "___Simplification___\n",
    "\n",
    "Given the transformation $ \\mathbf{h}' = \\sigma_v \\mathbf{h} $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ actually does not modify $ \\mathbf{h}' $ because it's directly aligned with $ \\mathbf{h} $. Thus, any projection onto $ \\mathbf{h} $ or away from $ \\mathbf{h} $ will not change $ \\mathbf{h}' $:\n",
    "\n",
    "- The expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ effectively simplifies to:\n",
    "  $ \\| \\sigma_v \\mathbf{h} \\|^2 $\n",
    "  by recognizing that $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' $ is simply $ \\mathbf{h}' $ itself (no component is removed by the projection):\n",
    "\n",
    "\n",
    "  $   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "  $ \\| \\sigma_v \\mathbf{h} \\|^2 = \\sigma_v^2 \\| \\mathbf{h} \\|^2 $\n",
    "\n",
    "___Conclusion___\n",
    "\n",
    "This simplification highlights that when $ R_{\\mathbf{VV}} = \\sigma_v^2 I_m $, the projection operator $ \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} $ retains the full transformed channel vector $ \\mathbf{h}' $ because the transformation does not introduce any components orthogonal to $ \\mathbf{h} $ itself. Therefore, the expression $ \\| \\mathit{P}_{\\mathbf{h}^{\\perp '}}^{\\perp} \\mathbf{h}' \\|^2 $ evaluates to $ \\sigma_v^2 \\| \\mathbf{h} \\|^2 $, representing the power of the channel in the presence of isotropic noise scaled by $ \\sigma_v^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd51b1-ba26-4b78-aafb-d7816621fc3d",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x62;)** ___LMS adaptation of $f$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca82880-fbd8-4b5c-9376-4c8962122c94",
   "metadata": {},
   "source": [
    "The [&#x1F449; LMS algorithm](lms.ipynb) consists of applying one iteration, per sampling period, of the steepest- descent strategy to the instantaneous error criterion $|\\epsilon[k](\\mathbf{f})|^2 = \\epsilon^*[k](\\mathbf{f}) \\, \\epsilon[k](\\mathbf{f})$. In the complex signals case, this becomes\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{f}[k] = \\left.\\mathbf{f}[k - 1] - \\mu \\frac{\\partial|\\epsilon[k](\\mathbf{f})|^2}{\\partial{\\mathbf{f^*}}}\\right|_{\\mathbf{f}=\\mathbf{f}[k-1]}\n",
    "\\end{equation}\n",
    "$$\n",
    "Work out the gradient term in this LMS update. We shall simplify the notation for the a priori error signal as $\\epsilon[k] = \\epsilon[k](\\mathbf{f}[k - 1])$.\n",
    "To check that the gradient has indeed to be taken w.r.t. $\\mathbf{f}^∗$ (and not $\\mathbf{f}$), express the a posteriori error signal $\\epsilon[k](\\mathbf{f}[k])$ as a function of the a priori error signal and observe that the update leads to a smaller error signal for a proper choice of stepsize $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780709f2-5917-4297-add3-c7b952f55153",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Derive the gradient term in the LMS (Least Mean Squares) algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5111f9-0375-4a86-bab3-4d9613e29fe5",
   "metadata": {},
   "source": [
    "To derive the Least Mean Squares (LMS) algorithm with respect to complex signals, particularly addressing the computation of the gradient term required for the update and the significance of using the conjugate transpose, here is a compact review:\n",
    "\n",
    "___Context and Equation___\n",
    "\n",
    "The LMS algorithm updates the filter coefficients by applying a steepest-descent method to minimize the instantaneous error criterion, which for complex signals involves the complex conjugate:\n",
    "\n",
    "$ |\\epsilon[k](\\mathbf{f})|^2 = \\epsilon^*[k](\\mathbf{f}) \\epsilon[k](\\mathbf{f}) $\n",
    "\n",
    "where the error $ \\epsilon[k] $ at iteration $ k $ is defined as:\n",
    "\n",
    "  $   \\framebox[1][10]{ a priori error signal : } $\n",
    "\n",
    "\n",
    "$ \\epsilon[k] = d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k] $\n",
    "\n",
    "Here, $ d[k] $ is the desired signal, $ \\mathbf{x}[k] $ is the input, and $ \\mathbf{f}[k-1] $ are the filter coefficients from the previous iteration.\n",
    "\n",
    "___Gradient Computation___\n",
    "\n",
    "To update the coefficients effectively, the gradient of the error squared with respect to the conjugate of the filter coefficients ($\\mathbf{f}^*$) must be computed. This is because the filter output depends linearly on the conjugate of the coefficients in the complex-valued domain.\n",
    "\n",
    "The gradient of $ |\\epsilon[k]|^2 $ with respect to $ \\mathbf{f}^* $ is:\n",
    "\n",
    "$ \\frac{\\partial |\\epsilon[k]|^2}{\\partial \\mathbf{f}^*} = \\frac{\\partial}{\\partial \\mathbf{f}^*} \\left((d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])^*(d[k] - \\mathbf{f}[k-1] \\mathbf{x}[k])\\right) $\n",
    "\n",
    "Breaking down the product and applying the derivative yields:\n",
    "\n",
    "$ \\frac{\\partial |\\epsilon[k]|^2}{\\partial \\mathbf{f}^*} = -\\mathbf{x}[k] \\epsilon^*[k] $\n",
    "\n",
    "___Update Rule___\n",
    "\n",
    "Incorporating the derived gradient into the update equation provides the rule for adjusting the filter coefficients:\n",
    "\n",
    "  $   \\framebox[1][10]{ update equation solution: } $\n",
    "\n",
    "$ \\mathbf{f}[k] = \\mathbf{f}[k-1] + \\mu \\epsilon[k] \\mathbf{x}^H[k] $\n",
    "\n",
    "This update ensures the filter coefficients are adjusted in a manner that minimizes the mean squared error. The term $\\mathbf{x}^H[k]$ represents the conjugate transpose of the input vector, which correctly aligns the dimensions and conjugate pairs for the update in the complex domain.\n",
    "\n",
    "___Validation of the Update___\n",
    "\n",
    "To validate that this update leads to a reduction in the error for an appropriate choice of step size ($\\mu$), consider the a posteriori error signal after applying the update:\n",
    "\n",
    "   $ \\epsilon[k](\\mathbf{f}[k]) = d[k] - \\mathbf{f}[k] \\mathbf{x}[k] $\n",
    "\n",
    "   Substituting the update equation:\n",
    "\n",
    "   $ \\epsilon[k](\\mathbf{f}[k]) = d[k] - (\\mathbf{f}[k-1] + \\mu \\epsilon[k] \\mathbf{x}^H[k]) \\mathbf{x}[k] $\n",
    "\n",
    "   $ \\framebox[1][10]{ a posteriori error signal solution: } $\n",
    "\n",
    "   $ \\epsilon[k](\\mathbf{f}[k]) = \\epsilon[k] - \\mu \\epsilon[k] \\|\\mathbf{x}[k]\\|^2 = ( 1 - \\mu \\|\\mathbf{x}[k]\\|^2 ) \\epsilon[k] $\n",
    "\n",
    "   The term $\\mu \\epsilon[k] \\|\\mathbf{x}[k]\\|^2$ indicates that if $\\mu$ is chosen small enough, the error magnitude $|\\epsilon[k](\\mathbf{f}[k])|^2$ can indeed be reduced compared to $|\\epsilon[k]|^2$, confirming the efficacy of the update rule in reducing the error. This adjustment aligns with the steepest descent approach, optimized for complex signals to ensure convergence towards the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73706076-74b0-47e4-b005-3f4745c42447",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x63;)** ___Steady-state analysis of LMS adaptation of $\\mathbf{f}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210e57-0550-442c-b96c-8cbd624a8e12",
   "metadata": {},
   "source": [
    "Let $\\mathbf{\\tilde{f}}[k] = \\mathbf{f}^o − \\mathbf{f}[k]$ be the filter error. Note that due to the presumed temporal whiteness of\n",
    "$a[k]$ and $\\mathbf{v}[k]$, also $d[k]$ and $\\mathbf{x}[k]$ are temporally white. Hence $\\mathbf{\\tilde{f}}[k−1]$ and $e[k]$ are independent\n",
    "(strictly speaking only uncorrelated).\n",
    "\n",
    "Let $R_{\\mathbf{\\tilde{f}}\\mathbf{\\tilde{f}}}[k] = \\mathrm{E} \\, \\mathbf{\\tilde{f}}^H[k] \\, \\mathbf{\\tilde{f}}[k]$. We can write for the a priori error signal and MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon[k] = e[k] + \\mathbf{\\tilde{f}}[k−1] \\mathbf{x}[k] \\implies \\sigma_{\\epsilon[k]}^2 = \\sigma_e^2 +tr\\{R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] \\, R_{\\mathbf{XX}} \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The LMS update (3) for $\\mathbf{f}$ leads to the following recursion for $\\mathbf{\\tilde{f}}[k]$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, \\mathbf{x}[k] \\, \\mathbf{x}^H[k]) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which, using the averaging analysis for small stepsize, can be approximated by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] ( I - \\mu \\, R_{\\mathbf{XX}}) - \\mu \\, e[k] \\, \\mathbf{x}^H[k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "From (6), obtain the time evolution for the filter error correlation matrix $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k]$. Assume\n",
    "$\\mu$ small so that $I − \\mu R_{\\mathbf{xx}}$ is stable and neglect second-order terms in $\\mu$. In steady-state $\\sigma_{\\epsilon[\\infty]}^2 = \\sigma_{\\epsilon}^2$ and $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[\\infty] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}$  . Show now from (6) that we obtain for the steady-state Excess MSE\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "EMSE = \\frac{\\mu}{2} \\, \\sigma_e^2 \\, tr\\{R_{\\mathbf{XX}} \\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note that $R_{\\mathbf{XX}} = \\mathbf{h}^{\\perp H} R_{\\mathbf{VV}} \\mathbf{h}^{\\perp}$ is not Toeplitz and its diagonal elements are not all equal. Using the simplified expression derived in `(a)` for $e[k]$, show the corresponding expression for the a priori error signal $\\epsilon[k]$.\n",
    "\n",
    "The signal part in $\\epsilon[k]$ is the term containing $a[k]$ and all the rest is noise. Using the expression for MMSE in `(2)`, derive an expression for the SNR in $\\epsilon[k]$. Note that the noise term contains a signal part which limits the SNR attainable by the adaptive system. This is due to the fact that the signal part in the error signal $e[k]$ acts like noise for the adaptation of the filter $\\mathbf{f}[k]$. This problem is generic for any adaptation algorithm and not just specific for LMS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f98572-236e-4c79-a699-b96f20c0d456",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** obtain the time evolution for the filter error correlation matrix $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k]$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca65072-4767-4ded-a519-efbad0756a27",
   "metadata": {},
   "source": [
    "To derive the time evolution of the filter error correlation matrix $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $ from the update equation of the filter error $ \\mathbf{\\tilde{f}}[k] $, we'll start by plugging the recursive definition into the expression for the correlation matrix, and then applying expectations to simplify terms.\n",
    "\n",
    "- ##### Step 1: Update Equation for $ \\mathbf{\\tilde{f}}[k] $\n",
    "Given:\n",
    "$ \n",
    "\\mathbf{\\tilde{f}}[k] = \\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\n",
    "$\n",
    "\n",
    "- ##### Step 2: Defining $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $\n",
    "The error correlation matrix $ R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] $ is defined as:\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \\mathrm{E}[\\mathbf{\\tilde{f}}[k] \\mathbf{\\tilde{f}}^H[k]]\n",
    "$\n",
    "\n",
    "- ##### Step 3: Substitute $ \\mathbf{\\tilde{f}}[k] $ into the Correlation Matrix\n",
    "Substituting the update equation into the correlation matrix definition:\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \\mathrm{E}\\left[\\left(\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\\right) \\left(\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) - \\mu e[k] \\mathbf{x}^H[k]\\right)^H\\right]\n",
    "$\n",
    "\n",
    "- ##### Step 4: Expand the Expectation\n",
    "\n",
    "Expanding the expression within the expectation:\n",
    "\n",
    "$ \n",
    "\\begin{flalign*}\n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = \n",
    "    \\mathrm{E}\\left[\\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) (I - \\mu R_{\\mathbf{XX}})^H \\mathbf{\\tilde{f}}^H[k - 1]\\right] \n",
    "    \\\\\n",
    "    - \\mathrm{E}\\left[\\mu \\mathbf{\\tilde{f}}[k - 1] (I - \\mu R_{\\mathbf{XX}}) e^*[k] \\mathbf{x}[k]\\right] \n",
    "    \\\\\n",
    "    - \\mathrm{E}\\left[\\mu e[k] \\mathbf{x}^H[k] (I - \\mu R_{\\mathbf{XX}})^H \\mathbf{\\tilde{f}}^H[k - 1]\\right] \n",
    "    \\\\\n",
    "    + \\mathrm{E}\\left[\\mu^2 e[k] e^*[k] \\mathbf{x}^H[k] \\mathbf{x}[k]\\right]\n",
    "\\end{flalign*}\n",
    "$\n",
    "\n",
    "- ##### Step 5: Simplify Using Independence and Neglecting Higher Order Terms\n",
    "Assuming $ \\mathbf{\\tilde{f}}[k - 1] $ and $ e[k] $ are uncorrelated and neglecting higher-order terms in $ \\mu $,\n",
    "\n",
    "  $   \\framebox[1][10]{ the equation simplifies to: } $\n",
    "\n",
    "\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] - \\mu R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}} - \\mu R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] + \\mu^2 \\sigma_e^2 R_{\\mathbf{XX}}\n",
    "$\n",
    "\n",
    "Neglegted term: $+ \\mu^2 R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}}$ 4th term\n",
    "\n",
    "### Conclusion\n",
    "This equation describes how the error correlation matrix evolves over time under the influence of the input correlation matrix $ R_{\\mathbf{XX}} $ and the learning rate $ \\mu $. It provides insight into the stability and convergence characteristics of the adaptive filter, highlighting the impact of step size and input properties on the filter's learning dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db7fa3-fff1-4fb3-88bd-7f1887de682b",
   "metadata": {},
   "source": [
    "From the update equation for $\\mathbf{\\tilde{f}}[k]$ we can obtain an update equation for $\\mathbf{\\tilde{f}}^H[k]\\mathbf{\\tilde{f}}[k]$ and take expectation, which yields:\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] - \\mu R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}} - \\mu R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] + \\mu^2 R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] R_{\\mathbf{XX}} + \\mu^2 \\sigma_e^2 R_{\\mathbf{XX}}\n",
    "$\n",
    "\n",
    "which becomes in steady state where $R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}[k - 1] = R_{\\mathbf{\\tilde{f}\\tilde{f}}}$\n",
    "\n",
    "\n",
    "$ \n",
    "R_{\\mathbf{\\tilde{f}\\tilde{f}}} = R_{\\mathbf{\\tilde{f}\\tilde{f}}} - \\mu R_{\\mathbf{\\tilde{f}\\tilde{f}}} R_{\\mathbf{XX}} - \\mu R_{\\mathbf{XX}} R_{\\mathbf{\\tilde{f}\\tilde{f}}} + \\mu^2 \\sigma_e^2 R_{\\mathbf{XX}}\n",
    "$\n",
    "\n",
    "from which we can easily solve for the desired expression for the \n",
    "\n",
    "$ \n",
    "EMSE = tr\\{R_{\\mathbf{\\mathbf{\\tilde{f}\\tilde{f}}}} R_{\\mathbf{XX}} \\} = tr\\{ R_{\\mathbf{XX}} R_{\\mathbf{\\mathbf{\\tilde{f}\\tilde{f}}}} \\}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826ed87-ca8a-42c0-8503-a39929ee9932",
   "metadata": {},
   "source": [
    "\n",
    "- **Updated Equation**: Focuses on how the error evolves and reacts to ongoing learning and adaptation, used during the active phase of filter training and initial deployment.\n",
    "- **Steady-State Equation**: Provides insights into the performance and behavior of the filter after it has adapted sufficiently to the statistics of the inputs and noise, important for evaluating final filter settings and design parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee90f58-beee-419d-887b-688e66cecb98",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x64;)** ___Steady-state analysis of signal compensated LMS adaptation of $\\mathbf{f}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f70cf-4f6a-4aa3-a036-13eee94f6522",
   "metadata": {},
   "source": [
    "Consider now compensating the signal part in the desired response signal $d[k]$ for the LMS adaptation. So we shall take as desired response $d[k] = \\mathbf{h}^H (\\mathbf{y}[k] − \\mathbf{h} \\, a[k]) = \\mathbf{h}^H \\, \\mathbf{v}[k]$. The goal of the receiver is to detect the $a[k]$ which are hence unknown. The way this signal compensation can be implemented then is by either limiting the update for f to time instants at which the $a[k]$ are training symbols (used for estimating h also, see further) or by using the detected $a[k]$ (decision-directed (DD) strategy). In the DD strategy, the symbol $a[k]$ gets detected from $\\mathbf{h}^H \\, \\mathbf{y}[k] − \\mathbf{f}[k - 1]$ (or delay needs to be introduced for the updating of f if also channel decoding gets exploited to get more reliable $a[k]$). Making abstraction of these details, consider hence $d[k] = \\mathbf{h}^H \\, \\mathbf{v}[k]$.\n",
    "\n",
    "Does the signal compensation influence the optimal filter setting $\\mathbf{f}^o$? What do the optimal error signal $e[k]$ and associated MMSE $\\sigma_e^2$ become?\n",
    "\n",
    "The signal compensation only gets done for the adaptation of $\\mathbf{f}$. The thus adapted $\\mathbf{f}$ then gets used in the original ICMF circuit. So, at the output of the ICMF, with the adapted $\\mathbf{f}$, what does the SNR become? With the signal compensation, the SNR degradation due to the adaptation of $\\mathbf{f}$ can be made arbitrarily small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a569-d0fa-4dfe-adbe-10c71c84c142",
   "metadata": {},
   "source": [
    "**&#x1F4DD;** Does the signal compensation influence the optimal filter setting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad6d78-c0d2-4aba-98a8-5edbd3b8f5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6107eb07-34d6-4669-a68c-00374a4b2742",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x65;)** ___LMS adaptation of $\\mathbf{h}$___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12719ec-7869-4f17-a13a-2bd5d5fc8209",
   "metadata": {},
   "source": [
    "The transmitted symbols are in fact partitioned into known training symbols and actual data symbols. The training symbols get inserted periodically. They get used to adapt the channel estimate. From now on we shall denote the true value of the channel as ho (assumed time-invariant). For the adaptation of the channel estimate, consider the error signal $\\mathbf{w}[k](\\mathbf{h}) = \\mathbf{y}[k] − \\mathbf{h} \\, a[k]$. The optimal value for the error signal $\\mathbf{w}[k](\\mathbf{h}^o)$ has already been specified in the problem formulation. What is it?\n",
    "\n",
    "The LMS algorithm performs one iteration of the steepest-descent strategy per training symbol to the instantaneous error criterion $\\|\\mathbf{w}[k](\\mathbf{h})\\|^2 = \\mathbf{w}^H[k](h) \\, \\mathbf{w}[k](\\mathbf{h})$. Derive the LMS algorithm that updates the channel estimate $\\mathbf{h}[k]$ (which could have been denoted also as $\\mathbf{\\hat{h}}[k]$, but let’s keep $\\mathbf{h}[k]$). Denote the a priori error signal as $\\mathbf{w}[k]$ and the stepsize as $\\nu$. Note that the time index now is no longer the true time index but a counter for the training symbols only, since adaptation occurs only when a symbol is a training symbol.\n",
    "Develop the recursion for the channel estimation error $\\mathbf{\\hat{h}}[k] = \\mathbf{h}^o − \\mathbf{h}[k]$. Find the steady-state value for $R_\\mathbf{\\hat{h}\\hat{h}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520b431-45dc-4421-9830-2eff1a3c813c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e626d72c-7e8c-47c6-aeca-e75f92cf93ce",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x66;)** ___Effect of channel adaptation on LMMSE ICMF operation with long-term IC estimation___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfafe8-4c76-4e2c-a719-48982099f147",
   "metadata": {},
   "source": [
    "Consider now the use of the adapted $\\mathbf{h}[k]$ in the ICMF: for any data symbol $a[k]$, the $\\mathbf{h}$ that will be used is the one adapted with LMS at the latest training symbol before the current data symbol. The main effect is that the channel estimation error $\\mathbf{\\tilde{h}}$ will lead to signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter $\\mathbf{h}^{\\perp H}$. The effect of the error $\\mathbf{\\tilde{h}}$ on $\\mathbf{h}^\\perp$ will depend on the choice of $\\mathbf{h}^\\perp$. Assuming the error to be small, we can perform a first-order analysis of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "0 = \\mathbf{h}^{\\perp H} \\, \\mathbf{h} = (\\mathbf{h}^{o \\perp} − \\mathbf{\\tilde{h}}^{\\perp})^H (\\mathbf{h}^o − \\mathbf{\\tilde{h}}) \\approx \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}} − \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\implies \\mathbf{\\tilde{h}}^{\\perp H} \\, \\mathbf{h}^o \\approx - \\mathbf{h}^{o \\perp H}\\mathbf{\\tilde{h}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\tilde{h}}^{\\perp}$ is not the orthogonal complement of $\\mathbf{\\tilde{h}}$  but the error on $\\mathbf{h}^{\\perp}$.\n",
    "\n",
    "Describe the signals $d[k]$ and $\\mathbf{x}[k]$ for the ICMF operation in terms of $\\mathbf{h}^o$, $\\mathbf{\\tilde{h}}$ and their orthogonal complement versions, and $\\mathbf{a}[k]$ and $\\mathbf{v}[k]$, neglecting products of noise terms, and using `(8)`.\n",
    "\n",
    "Find $R_{d\\mathbf{X}}$ and $R_\\mathbf{XX}$. Find the LMMSE filter $\\mathbf{f}$ in terms of the unperturbed version $\\mathbf{f}^o$.\n",
    "\n",
    "Express the corresponding error signal $e[k]$, and MMSE in terms of $\\mathbf{h}^{'} = R_\\mathbf{VV}^{H/2} \\mathbf{h}^o , \\mathbf{h}^{\\perp '} =\n",
    "R_\\mathbf{VV}^{H/2} \\mathbf{h}^{o \\perp}$. Give the increase in MSE due to the channel estimation error. How much is this increase when $R_\\mathbf{VV} = \\sigma_v^2I_m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a9081-50a8-4d10-bb91-20636a82e4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9866d36-28f4-40a9-8b42-d5ca48a8836e",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x67;)** ___Effect of channel adaptation on LMMSE ICMF operation with short-term IC estimation___\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b79eec-07ac-4f35-931c-87d23528024e",
   "metadata": {},
   "source": [
    "In `(f)`, we considered the effect of channel estimation error on the operation of the ICMF when the Interference Canceling (IC) filter $\\mathbf{f}$ is adapted with long-term statistics $R_{d \\mathbf{X}}$, $R_\\mathbf{XX}$. In that case, statistical averaging occurs not only over the noise and the transmitted data but also over the channel estimation error since many instances of this error will be involved and get averaged out. Another possible configuration is shorttime averaging for $\\mathbf{f}$, involving essentially the data between two training symbols, so that the channel estimation error remains constant in such a period. This short-term averaging only averages over noise and transmitted data. The signal leakage in the output $\\mathbf{x}[k]$ of the blocking filter $\\mathbf{h}^{\\perp H}$ will now lead to correlation between $\\mathbf{d}[k]$ and $\\mathbf{x}[k]$.\n",
    "\n",
    "Take again the signal descriptions for $d[k]$ and $\\mathbf{x}[k]$ from `(f)`, up to first order in $\\mathbf{\\tilde{h}}$. Find $R_{d \\mathbf{X}}$ and $R_\\mathbf{XX}$ using averaging over noise and symbols only, up to first order in $\\mathbf{\\tilde{h}}$. Find the LMMSE filter $\\mathbf{f}$ up to first order in $\\mathbf{\\tilde{h}}$, in terms of the unperturbed version $\\mathbf{f}^o$. Note that the perturbation in $\\mathbf{f}$ due to $\\mathbf{\\tilde{h}}$ is proportional to signal power $\\sigma_a^2\\|\\mathbf{h}^o\\|^2$.\n",
    "\n",
    "Express the corresponding error signal $e[k] = d[k] − \\mathbf{f x}[k]$ in terms of the unperturbed $e^o[k]$ and first-order perturbation terms in $\\mathbf{\\tilde{h}}$. Note that the perturbation terms are mutually uncorrelated. Why?\n",
    "\n",
    "Compute the corresponding MMSE, $E \\| e[k] \\|^2$, by now also averaging over $\\mathbf{\\tilde{h}}$, to get a simplified average expression, assuming the LMS adaptation for $\\mathbf{h}[k]$ as in `(e)`.\n",
    "\n",
    "The signal part in $e[k]$ that the receiver will assume on the basis of its knowledge of $\\mathbf{h} = \\mathbf{h}^o − \\mathbf{\\tilde{h}}$ is $\\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ while hence $e[k] − \\| \\mathbf{h}^o − \\mathbf{\\tilde{h}} \\|^2 a[k]$ is considered noise. Compute the resulting SNR with numerator and denominator averaged over $\\mathbf{\\tilde{h}}$ and computed up to first order in $\\nu$. The channel estimation error leads to signal leakage in the bottom branch of the ICMF, which leads to some _signal cancellation_ and ensuing loss in SNR. In the normal Generalized Sidelobe Canceler (GSC), of which the ICMF is a special instance, any error in the blocking filter ($\\mathbf{h}^{\\perp}$ in the ICMF case) leads to signal cancellation that becomes total when the received signal SNR increases (hence the SNR at the output of the GSC goes to zero then). In our analysis the ICMF output SNR remains bounded away from zero due to the fact that as the received SNR increases, the channel estimation error decreases also. Note the similarity with the loss in SNR in `(c)` due to IC filter adaptation by LMS without signal compensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8d7de-9633-4c3d-830b-5ada5af4f250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
