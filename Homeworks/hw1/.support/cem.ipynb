{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning with Cross-entropy method optimization (CEM)\n",
    "\n",
    "CEM performs a policy search with exploration directly in parameter space. A collection of sample policies are sampled from a distribution and tried through experiments. A new, weighted distribution is fit to the sampled policy parameters, where the reward achieved by the policy determines its weight. This way, policies that achieved high rewards are more likely to be sampled next time. This procedure is iterated until performance is adequate.\n",
    "\n",
    "CEM works well when simulation is computationally cheap and the parameter dimension is small. We therefore use a linear combination of the states as policy in this example, since the linear combination of basis function leads to a high-dimensional parameter space.\n",
    "\n",
    "We focus on Gaussian distributions to represent policy distributions in parameter space, and illustrate two methods of fitting new weighted distributions: rank-based and Boltzmann-based. The rank-based fit fits and unweighted Gaussian to the top fraction of policies, whereas the Boltzmann-based strategy assigns a weight of $\\exp(r_i)$ to policy $i$, where $r_i$ is the total reward achieved by that policy.\n",
    "\n",
    "For further introduction, see [Levine 2017 (slides)](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_7_advanced_q_learning.pdf)\n",
    "\n",
    "This tutorial is available as an [iJulia notebook](https://github.com/baggepinnen/baggepinnen.github.io/blob/master/cem.ipynb)\n",
    "\n",
    "We start out by importing some packages. If it's your first time, you might have to install some packages using the commands in the initial comment.\n",
    "OpenAI gym is installed with instructions available at https://github.com/JuliaML/OpenAIGym.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"BasisFunctionExpansions\")\n",
    "# Pkg.add(\"ValueHistories\")\n",
    "# Pkg.add(\"StatsBase\"); Pkg.add(\"Distributions\")\n",
    "# Pkg.add(\"https://github.com/JuliaML/OpenAIGym.jl\")\n",
    "\n",
    "\n",
    "# using OpenAIGym\n",
    "using ValueHistories, Plots, LinearAlgebra, Random, Statistics, StatsBase, Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform plotting in the loop, the following function helps keeping the plot clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_plot! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default(size=(1200,800)) # Set the default plot size\n",
    "function update_plot!(p; max_history = 10)\n",
    "    num_series = length(p.series_list)\n",
    "    if num_series > 1\n",
    "        if num_series > max_history\n",
    "            deleteat!(p.series_list,1:num_series-max_history)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an environment from the OpenAIGym framework, we'll use the [cartpole environment](https://gym.openai.com/envs/CartPole-v0/) and a function that runs an entire episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `GymEnv` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `GymEnv` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:1"
     ]
    }
   ],
   "source": [
    "env = GymEnv(\"CartPole-v0\")\n",
    "\n",
    "function collect_reward(ep)\n",
    "    r = Vector{Float64}()\n",
    "    for (ss,aa,rr,ss1) in ep\n",
    "        push!(r,rr)\n",
    "    end\n",
    "    r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a linear policy object that is a linear combination of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Policy <: AbstractPolicy\n",
    "    θ::Vector{Float64}\n",
    "end\n",
    "\n",
    "(π::Policy)(s) = π.θ's + 0.5 > 0.5 ? 1 :  0 # policy function\n",
    "\n",
    "Reinforce.action(π::Policy, r, s, A) = π(s)\n",
    "\n",
    "num_params  = length(env.state)\n",
    "num_epochs  = 15\n",
    "num_samples = 25\n",
    "const π     = Policy(zeros(num_params)) # π is now our policy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now arrived at the main algorithm. We wrap it in a function for the Julia JIT complier to have it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit_new_distribution_weighted(θs, w)\n",
    "    weights = ProbabilityWeights(exp.(w))\n",
    "    μ       = mean(θs, weights, dims=2)[:]\n",
    "    Σ       = cov(θs, weights, 2, corrected=true) + 1e-5I\n",
    "    MvNormal(μ, Σ)\n",
    "end\n",
    "\n",
    "function fit_new_distribution_rank(θs, w)\n",
    "    fraction = 2\n",
    "    p        = sortperm(w)\n",
    "    indicies = p[end-end÷fraction:end]\n",
    "    μ        = mean(θs[:,indicies], dims=2)[:]\n",
    "    Σ        = cov(θs[:,indicies], dims=2) + 1e-5I\n",
    "    MvNormal(μ, Σ)\n",
    "end\n",
    "\n",
    "gr() # Set GR as plot backend, try pyplot() if GR is not working\n",
    "function CME(π, num_epochs, num_samples; plotting=true)\n",
    "    dist = MvNormal(zeros(num_params), 0.2Matrix{Float64}(I,num_params,num_params)) # Initial noise\n",
    "    plotting && (fig = plot(layout=2, show=true))\n",
    "    reward_history = ValueHistories.History(Float64)\n",
    "    for i = 1:num_epochs\n",
    "        θs = rand(dist, num_samples) # Sample new policies\n",
    "        weights = zeros(num_samples)\n",
    "        for n = 1:num_samples # Collect N trajectories for Expectation estimation\n",
    "            π.θ       .= θs[:,n] # Set policy parameters\n",
    "            ep         = Episode(env, π)\n",
    "            s,a,r,s1   = collect_reward(ep)\n",
    "            weights[n] = ep.total_reward\n",
    "            push!(reward_history, (i-1)*num_samples+n, ep.total_reward)\n",
    "        end\n",
    "        dist = fit_new_distribution(θs, weights)\n",
    "\n",
    "        # Printing and plotting\n",
    "        println(\"Epoch: $i, reward: $(reward_history.values[end])\")\n",
    "        if plotting\n",
    "            plot!(reward_history, subplot=1, show=false)\n",
    "            scatter!(θs, subplot=2, c=:red, title=\"Policy parameters\", show=false, legend=false)\n",
    "            update_plot!(fig[1], max_history=1)\n",
    "            update_plot!(fig[2], max_history=num_samples)\n",
    "            gui(fig)\n",
    "        end\n",
    "    end\n",
    "    plot(reward_history, title=\"Rewards\", xlabel=\"Episodes\", show=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call our function, with the two different distribution-refit strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_new_distribution = fit_new_distribution_weighted\n",
    "Random.seed!(0);\n",
    "CME(π, num_epochs, num_samples; plotting=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_new_distribution = fit_new_distribution_rank\n",
    "Random.seed!(0);\n",
    "CME(π, num_epochs, num_samples; plotting=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run many experiments, we can plot the average reward history. The following plot depicts the average over 50 parallel runs.\n",
    "![window](cem.png)\n",
    "As we can see, the weight-based method finds a good solution faster, but fails to zoom in on a perfect policy like the rank-based method does after a while. Note, very little time was spent on tuning the hyper parameters of the two methods. The Boltzmann method can be tuned with a temperature parameter that controls the entropy of the distribution, this will allow for also this method to zoom in on a good policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
