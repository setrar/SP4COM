{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc12c66-7495-4381-9794-c60813c6940a",
   "metadata": {},
   "source": [
    "SIMD (Single Instruction, Multiple Data) programming is a technique used to achieve data level parallelism in computing. It's a class of parallel computing where a single operation can be performed on multiple data points simultaneously. This method is particularly effective for tasks that require the same operation to be applied to a large set of data, such as image and signal processing, scientific simulations, and financial analysis.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Vectorization**: The process of converting algorithm operations to run on vectors (arrays of data) rather than on a single piece of data at a time. This is a fundamental concept in SIMD programming, as operations are applied to vectors of data in parallel.\n",
    "\n",
    "- **SIMD Instructions**: Modern CPUs and GPUs contain special instruction sets designed for parallel operations on multiple data points. Examples include Intel's SSE (Streaming SIMD Extensions) and AVX (Advanced Vector Extensions), and ARM's NEON for mobile processors.\n",
    "\n",
    "- **Parallelism**: SIMD exploits data parallelism by applying the same operation to multiple data points in parallel, significantly speeding up computation for tasks with high data parallelism.\n",
    "\n",
    "- **Performance Gains**: SIMD can lead to significant performance improvements, especially in applications that process large amounts of data, by utilizing the CPU or GPU's parallel processing capabilities effectively.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "In traditional scalar computing, a CPU performs operations on one data point at a time. For instance, if you wanted to add two arrays of numbers, a scalar processor would add the first elements, then the second elements, and so on, in a sequential manner.\n",
    "\n",
    "In contrast, SIMD allows for multiple data points to be processed simultaneously. Using the same example, a SIMD-enabled processor could add corresponding elements from the two arrays in parallel, significantly reducing the total computation time for the entire arrays.\n",
    "\n",
    "### Programming with SIMD\n",
    "\n",
    "- **High-Level Languages**: Many high-level programming languages and libraries offer ways to utilize SIMD without needing to write low-level code. For example, Python's NumPy library can automatically use SIMD where possible. Other languages like C++ offer libraries like Eigen or Intel's MKL for SIMD operations.\n",
    "\n",
    "- **Intrinsic Functions**: For more control, some developers use intrinsic functions provided by the processor's architecture. These are low-level functions that map directly to SIMD instructions, offering precise control over how data is processed.\n",
    "\n",
    "- **Compiler Auto-Vectorization**: Modern compilers can automatically vectorize code to some extent. This means the compiler identifies opportunities to use SIMD instructions and applies them without the programmer needing to manually optimize the code for SIMD.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- **Portability**: Code optimized for one set of SIMD instructions might not perform well on a processor with a different set of instructions. Developers often need to write different code paths for different architectures.\n",
    "\n",
    "- **Complexity**: Writing efficient SIMD-optimized code can be complex and requires a deep understanding of the target processor's architecture and instruction set.\n",
    "\n",
    "SIMD programming is a powerful tool for enhancing performance in applications that can leverage parallel processing of data. However, it requires careful consideration of the specific requirements and characteristics of the application and the target hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e6111-4ff3-419f-b6e9-9e240dc27e0f",
   "metadata": {},
   "source": [
    "Programming with SIMD in Julia offers a powerful way to leverage the hardware's parallel processing capabilities for significant performance improvements in numerical and data-intensive applications. Julia, being a high-level, high-performance language designed for technical computing, integrates well with SIMD concepts, enabling both automatic and manual optimization techniques.\n",
    "\n",
    "### Automatic SIMD in Julia\n",
    "\n",
    "Julia's compiler can automatically vectorize loops to use SIMD instructions where it deems beneficial. This automatic vectorization is part of Julia's just-in-time (JIT) compilation process, which generates optimized machine code tailored to the specific architecture of the host CPU. To benefit from automatic SIMD vectorization:\n",
    "\n",
    "1. **Write Fast, Type-Stable Code**: Ensure your Julia code is type-stable and avoid type ambiguities. The Julia compiler can optimize type-stable loops much more effectively.\n",
    "2. **Use Built-in Functions**: Julia's standard library functions are often already optimized to use SIMD where appropriate. Using these functions can sometimes yield better performance than manually writing loop-based code.\n",
    "3. **Annotations and Pragmas**: While Julia attempts to automatically vectorize code, you can give the compiler hints using pragmas like `@simd` for loops. This tells the compiler that the loop iterations are independent and can safely be executed in parallel, making it a candidate for SIMD optimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a3a339-65f0-4787-95ff-0a05c9372206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simd_example (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Base.Threads: @threads\n",
    "\n",
    "function simd_example()\n",
    "    a = rand(10^7)  # Large array\n",
    "    b = rand(10^7)\n",
    "    c = zeros(10^7)\n",
    "\n",
    "    @simd for i in 1:length(a)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "\n",
    "    return c\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b592ef-2c03-4d42-ac6f-85ed677b2084",
   "metadata": {},
   "source": [
    "### Manual SIMD Programming in Julia\n",
    "\n",
    "For cases where automatic optimization isn't enough, Julia provides more direct access to SIMD capabilities through packages that expose SIMD instructions and data types:\n",
    "\n",
    "- **SIMD.jl**: This package allows you to work with SIMD vectors explicitly in Julia. It provides types and functions that map closely to the hardware's SIMD features, enabling you to write highly optimized code that directly uses SIMD instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7373cf-fed1-4f70-9342-daf413f4192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SIMD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c49fb38-6911-4c93-a034-cff2a69645fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manual_simd_example (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function manual_simd_example()\n",
    "    a = Vec{4, Float64}((rand(), rand(), rand(), rand()))\n",
    "    b = Vec{4, Float64}((rand(), rand(), rand(), rand()))\n",
    "    c = a + b  # SIMD addition\n",
    "    return c\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376009f-3cb4-4810-ba84-f9ac7e24a9b7",
   "metadata": {},
   "source": [
    "### Considerations and Best Practices\n",
    "\n",
    "- **Testing and Validation**: Ensure your SIMD-optimized code is thoroughly tested. SIMD operations can introduce subtle bugs, especially in complex numerical computations.\n",
    "- **Benchmarking**: Use Julia's BenchmarkTools.jl package to benchmark your SIMD-optimized code against its scalar counterpart to measure actual performance gains.\n",
    "- **Hardware Specificity**: Write and test your SIMD code on the hardware where it will be deployed. SIMD performance can vary significantly across different processors.\n",
    "- **Use Libraries When Possible**: Leverage existing Julia libraries that are already optimized for SIMD, such as those for linear algebra, image processing, and statistics, to avoid reinventing the wheel.\n",
    "\n",
    "SIMD programming in Julia strikes a balance between high-level ease of use and low-level hardware control, making it an appealing choice for performance-critical applications in scientific computing, data analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdd6af-659a-414a-9ae3-cc51ebdbe5df",
   "metadata": {},
   "source": [
    "### Basic Julia Loop for Array Addition\n",
    "Here's a straightforward function that adds two arrays element-wise. This version doesn't explicitly use SIMD instructions but might still be auto-vectorized by Julia's compiler if it finds it beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b879125d-76a2-4a94-ae0a-79614a1a9885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_arrays_basic (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_arrays_basic(a::Vector{Float64}, b::Vector{Float64}) :: Vector{Float64}\n",
    "    length(a) == length(b) || throw(DimensionMismatch(\"Arrays must have the same length\"))\n",
    "    result = Vector{Float64}(undef, length(a))\n",
    "    for i in eachindex(a, b)\n",
    "        result[i] = a[i] + b[i]\n",
    "    end\n",
    "    return result\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25e6dc-51e1-4b8d-8072-0c11c5561b7f",
   "metadata": {},
   "source": [
    "### SIMD-Optimized Array Addition with SIMD.jl\n",
    "Now, let's rewrite the above function using the SIMD.jl package to manually vectorize the loop. This version will use SIMD operations to perform the addition, likely resulting in significant performance improvements for large arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8ca7fd-ba7f-4f14-bdd1-12e565b8a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SIMD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19da397-e912-4876-bf81-7588b84cef4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_arrays_simd (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_arrays_simd(a::Vector{Float64}, b::Vector{Float64}) :: Vector{Float64}\n",
    "    length(a) == length(b) || throw(DimensionMismatch(\"Arrays must have the same length\"))\n",
    "    result = Vector{Float64}(undef, length(a))\n",
    "    len = length(a)\n",
    "    \n",
    "    # Determine the SIMD vector length\n",
    "    simd_width = Val{4}()  # Example for Float64, adjust based on your CPU capabilities and data type\n",
    "    simd_len = len ÷ SIMD.width(simd_width)\n",
    "\n",
    "    @inbounds for i in 1:simd_len\n",
    "        ai = vload(Vec{4, Float64}, a, (i-1) * SIMD.width(simd_width) + 1)\n",
    "        bi = vload(Vec{4, Float64}, b, (i-1) * SIMD.width(simd_width) + 1)\n",
    "        ci = ai + bi\n",
    "        vstore(ci, result, (i-1) * SIMD.width(simd_width) + 1)\n",
    "    end\n",
    "\n",
    "    # Handle any remaining elements that didn't fit into a full SIMD vector\n",
    "    @inbounds for i in simd_len * SIMD.width(simd_width) + 1:len\n",
    "        result[i] = a[i] + b[i]\n",
    "    end\n",
    "\n",
    "    return result\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc3b8-fbd6-488c-af84-89a75b2bd1e4",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- The `vload` function loads contiguous elements from the arrays into SIMD vectors.\n",
    "- The addition (`ai + bi`) is performed using SIMD operations.\n",
    "- The `vstore` function stores the result of the SIMD operation back into the result array.\n",
    "- The loop handles the main chunk of the array that can be evenly divided by the SIMD width, while the final loop takes care of any remaining elements.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "To compare the performance of both approaches, you can create two large arrays and use Julia's `@time` macro to measure the execution time of each function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e797358e-95bf-4eb4-9511-c2350688dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.020220 seconds (2 allocations: 76.294 MiB, 26.94% gc time)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `width` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `width` not defined",
      "",
      "Stacktrace:",
      " [1] add_arrays_simd(a::Vector{Float64}, b::Vector{Float64})",
      "   @ Main ./In[6]:8",
      " [2] macro expansion",
      "   @ ./timing.jl:279 [inlined]",
      " [3] top-level scope",
      "   @ ./In[7]:8"
     ]
    }
   ],
   "source": [
    "a = rand(Float64, 10^7)\n",
    "b = rand(Float64, 10^7)\n",
    "\n",
    "# Basic addition\n",
    "@time result_basic = add_arrays_basic(a, b)\n",
    "\n",
    "# SIMD-optimized addition\n",
    "@time result_simd = add_arrays_simd(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53084d-d89e-4393-88e0-245c4c843283",
   "metadata": {},
   "source": [
    "This comparison will highlight the performance benefits of using SIMD for data-parallel operations in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d62fdb-4856-48c0-a6af-519e84c45713",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b14a43-6a03-4023-8f43-825c20396771",
   "metadata": {},
   "source": [
    "- [ ] [Explicit SIMD vector operations for Julia](https://github.com/eschnett/SIMD.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d16811-0074-4482-a9b7-7ef23d093f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
